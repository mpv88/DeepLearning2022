{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYFy9VDaj-ye"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lorenzobasile/DeepLearning2022/blob/main/4_cnn.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xp7M6MNej-yh"
   },
   "source": [
    "# Lab 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBEJx3Lhj-yi"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "%cd drive/MyDrive/DeepLearning2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jcp8UmEJj-yk"
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4pnfKiOj-yl"
   },
   "source": [
    "As introduced in the previous lectures, Convolutional Neural Networks (CNNs) are the go-to architecture to deal with Computer Vision tasks, including image classification, segmentation and recognition.\n",
    "\n",
    "The main advantages of CNNs lie in the Convolutional Layer, that introduces a useful position-invariance inductive bias while keeping very limited the number of necessary parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fE4mNCmaj-yl"
   },
   "source": [
    "## The building blocks of CNNs\n",
    "\n",
    "### Convolutional layers\n",
    "\n",
    "The basic building block for a CNN is the convolutional layer, accessible as `torch.nn.Conv<s>d`, where `<s>` represents the number of **spatial dimensions** of our data:\n",
    "* `Conv1d` for 1 dimensional sequences. Example: audio. Audio is organized as a sequence of a given length (the single spatial dimension), where each single value in this sequence represent the intensity/amplitude of the signal for a given time point. Audio data can be organized in multiple **channels** (e.g., stereo data has 2 channels). The convolution operation is represented by a one-dimensional kernel;\n",
    "* `Conv2d` for 2 dimensional data, like images.\n",
    "* `Conv3d` for 3 dimensional data. An example might be a 3D reconstruction of an image. A convolution in that domain might equate to sliding a cubic kernel along all three dimensions.\n",
    "\n",
    "(Some) parameters for constructors:\n",
    "```\n",
    "Conv2d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int]] = 0)\n",
    "```\n",
    "* in_channels: the number of channels of the incoming data\n",
    "* out_channels: the number of channels for the output data, i.e., the number of convolutions that are operated\n",
    "* kernel_size: the kernel size of each convolution. An int $k$ is interpreted as a tuple $(k, k)$ (i.e., a square kernel); for a rectangular kernel, pass a tuple.\n",
    "* stride: the step used when moving the kernel on the input data\n",
    "* padding: if set to >0, the incoming image is enlarged with `padding` rows and columns of zeros (unless otherwise specified)\n",
    "\n",
    "To visualize these and other parameters and how they affect the convolution operation, please have a look at [this page](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md).\n",
    "\n",
    "#### Note that the convolution does NOT require a specific spatial dimension as input/output, as convolution is oblivious to these factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGUnwzkjj-ym"
   },
   "outputs": [],
   "source": [
    "\n",
    "conv_layer = torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "print(\"Parameters of convolution\\n\", \"Weights\\n\", conv_layer.weight.shape, \"\\nBias\\n\", conv_layer.bias.shape)\n",
    "\n",
    "print(\"Conv2d is applied independently of the input spatial dimension\")\n",
    "y = conv_layer(torch.rand(1,3,10,10))\n",
    "print(\"Shape of y \", y.shape)\n",
    "\n",
    "z = conv_layer(torch.rand(1,3,6,6))\n",
    "print(\"Shape of z \", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIC28u54j-yn"
   },
   "source": [
    "To better visualize how convolutions work with multi-channel data, have a look at [this](https://www.coursera.org/lecture/convolutional-neural-networks/convolutions-over-volume-ctQZz) short video by Andrew Ng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0Mn-4HRj-yo"
   },
   "source": [
    "### Pooling layers\n",
    "\n",
    "Pooling layers are essentially convolutions without trainable kernels. For each overlap between the image and the kernel, they output the maximum (→ _maxpooling_) or the average (→ _avgpooling_) of the image in that specific region.\n",
    "\n",
    "![](https://production-media.paperswithcode.com/methods/MaxpoolSample2.png)\n",
    "\n",
    "```MaxPool2d(kernel_size: Union[int, Tuple[int, ...]], stride: Union[int, Tuple[int, ...], NoneType] = None, padding: Union[int, Tuple[int, ...]] = 0)```\n",
    "\n",
    "Notice that now we have no input or output channels as parameter, because MaxPool/AvgPool act independently on each channel, so `in_channels=out_channels`\n",
    "\n",
    "#### Adaptive Pooling\n",
    "\n",
    "Adaptive (Max/Average) Pooling is still a pooling layer, but we have the option to specify the desired spatial dimension of the output instead of the parameters like kernel size, padding...\n",
    "\n",
    "PyTorch works out by itself the params which are required in order for the pooling to produce an output of the desired size.\n",
    "\n",
    "Maybe the most common application of this layer is when operating the channel-wise average pooling at the end of the cascade of convolutional layers. In this case, we specify a fixed size of $(1,1)$, s.t. PyTorch will essentially operate an average of each whole channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlpheY6zj-yp"
   },
   "outputs": [],
   "source": [
    "layer = torch.nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "layer(torch.rand(1,3,32,32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTkvRXxcj-yp"
   },
   "source": [
    "### Another minor \"layer\"\n",
    "\n",
    "To feed these data to a linear layer, we need one more building block: a flattening layer (actually more of an operation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sf72eXb8j-yp"
   },
   "outputs": [],
   "source": [
    "torch.nn.Flatten()(layer(torch.rand(1,3,32,32))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cceQAsP3j-yq"
   },
   "source": [
    "## Building a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNtM_e25_Ay2"
   },
   "source": [
    "We will work with a dataset we have already encountered: MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RE1FCoGQj-yq"
   },
   "outputs": [],
   "source": [
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST('./data/', transform=transforms,  train=True, download=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST('./data/', transform=transforms, train=False, download=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCbL3qa4_Po3"
   },
   "source": [
    "Our Convolutional Network follows a standard architectural paradigm: we have a sequence of interleaved convolutional and pooling layers followed by a fully-connected classification head.\n",
    "\n",
    "The thing we have to be the most careful about is the way data are reshaped by the conv and pooling layers, which depends on the parameters we set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzSBHdfLj-yq"
   },
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.conv = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=0),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.MaxPool2d(kernel_size=2),\n",
    "                torch.nn.Dropout(p=0.2),\n",
    "                torch.nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=2, padding=1),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.MaxPool2d(kernel_size=2,stride=1),\n",
    "                torch.nn.Flatten(),\n",
    "        )\n",
    "        self.head = torch.nn.Linear(8*6*6, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.head(self.conv(x))\n",
    "        \n",
    "        \n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s27FMQYGkUH4"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct=0\n",
    "        for x, y in iter(dataloader):\n",
    "            out=model(x)\n",
    "            correct+=(torch.argmax(out, axis=1)==y).sum()\n",
    "        return correct/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OL-R8lCbx30O"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, trainloader, testloader):\n",
    "    epochs=5\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Test accuracy: \", get_accuracy(model, testloader))\n",
    "        model.train()\n",
    "        print(\"Epoch: \", epoch)\n",
    "        for x, y in iter(trainloader):\n",
    "            out=model(x)\n",
    "            l=loss(out, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "    print(\"Final accuracy: \", get_accuracy(model, testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttulor0NkJo9"
   },
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss=torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ojDQqUCkWsi"
   },
   "outputs": [],
   "source": [
    "train(model, optimizer, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVpXtZB3A3Zv"
   },
   "source": [
    "Our CNN is performing better (with the same training length) than the MLP we trained a couple of labs ago.\n",
    "\n",
    "Somehow counterintuitively, the number of parameters of our current model is much smaller than it was for the MLP.\n",
    "\n",
    "What really makes CNNs so good at CV tasks is not the model complexity (number of parameters), but the inductive bias they insert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSopG68GmvRC"
   },
   "outputs": [],
   "source": [
    "def get_params_num(net):\n",
    "    return sum(map(torch.numel, net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PHttI9opTWY"
   },
   "outputs": [],
   "source": [
    "get_params_num(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-WG6ZFXAUJj"
   },
   "source": [
    "In the following part we will need to access the state of the network in which we are now (in terms of weights and bias values). To do so, we can simply save the state dictionary of the model to a file using the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sw7PIWjawJqM"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZrWYPQ9py19"
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "In Deep Learning practise, people very rarely train large models from scratch (i.e. from randomly initialized weights). This is especially true in Computer Vision, where pre-trained weights for many standard models are openly available online.\n",
    "\n",
    "In a usual setting, when facing a Computer Vision task (unless for some reason you want to use a customized architecture), you load a pre-trained model and **fine-tune** it.\n",
    "\n",
    "Fine-tuning can follow different paths: one possibility is to freeze all the layers of the network excluding the last one (the classification head), another is to train the whole end-to-end classifier starting from pre-determined weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IKm9NGY7DGJ"
   },
   "source": [
    "## Fine-tuning the whole network\n",
    "\n",
    "For our first example, we will see how to fine-tune an end-to-end classifier on a new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ovhGYXn0j42"
   },
   "source": [
    "### kMNIST\n",
    "\n",
    "For this example, we will work with kMNIST, a drop-in replacement for MNIST containing images of handwritten Kanji characters belonging to 10 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6X6q5_1p0L-"
   },
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "k_trainset = torchvision.datasets.KMNIST('./data/', transform=transforms,  train=True, download=True)\n",
    "k_trainloader = torch.utils.data.DataLoader(k_trainset, batch_size=256, shuffle=True)\n",
    "\n",
    "k_testset = torchvision.datasets.KMNIST('./data/', transform=transforms, train=False, download=True)\n",
    "k_testloader = torch.utils.data.DataLoader(k_testset, batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kj4HwW6B08Kf"
   },
   "outputs": [],
   "source": [
    "x,y=next(iter(k_trainloader))\n",
    "first_img=x[0]\n",
    "plt.imshow(first_img.reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHP_rFunuF-x"
   },
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffYs50UiyKPc"
   },
   "outputs": [],
   "source": [
    "train(model, optimizer, k_trainloader, k_testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyvgv76szXV7"
   },
   "source": [
    "## Feature extractor freezing\n",
    "\n",
    "Because of how similar our task is to MNIST classification, it may make sense to keep the feature extraction section of the network freezed (in our case, the convolutional part of the model), while training only the fully connected classification head. \n",
    "\n",
    "In this way we can make the training process lighter, reducing drastically the number of trainable parameters.\n",
    "\n",
    "First of all we have to restore the parameters we had after training on MNIST by loading them from memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tTglQJkwwd0"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"mnist_cnn.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCdrd4Bi9Lrl"
   },
   "source": [
    "Now we can turn off training for some layers by filtering them by name and setting to `False` the `requires_grad` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOibXj2ks-9h"
   },
   "outputs": [],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    print(name)\n",
    "    if \"head\" not in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgGxV_A2yYgM"
   },
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8D6bXp84yigP"
   },
   "outputs": [],
   "source": [
    "train(model, optimizer, k_trainloader, k_testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1pwqWLtyjIo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
