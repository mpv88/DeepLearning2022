{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hfdMFRBGZEo"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lorenzobasile/DeepLearning2022/blob/main/6_dql.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-U_0hnaeGZEp"
   },
   "source": [
    "# Lab 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jwP0YQgGZEq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuffuTU0GZEr"
   },
   "source": [
    "# An introducion to Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXWOXOXiGZEr"
   },
   "source": [
    "Today we will be seeing a simple example of how a Deep RL algorithm can help us understand how to navigate an initially unknown enviroment.\n",
    "\n",
    "Our task is usually referred to as *gridworld*, as we have an agent that moves in a rectangular grid with the final aim of collecting a **reward**.\n",
    "\n",
    "The agent knows its position, which is the **state** of our decision process and at each move it makes it receives a reward signal, which is either 0 if nothing happens, 1 if the reward is reached, and -2 if a penalty cell is reached.\n",
    "\n",
    "The agent is also penalized for the time it takes it to reach the objective. For this reason, the final goal would be to learn a way to reach the reward as quickly as possible, while avoiding the penalty cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlQORLYDGZEr"
   },
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    def __init__(self, nrows, ncols, target_pos, init_pos, penalty_pos=None):\n",
    "        self.dimensions=(nrows,ncols)\n",
    "        self.done=(target_pos==init_pos)\n",
    "        self.target=target_pos\n",
    "        self.current_pos=init_pos\n",
    "        self.penalty=penalty_pos\n",
    "        \n",
    "    def observe(self):\n",
    "        return self.current_pos\n",
    "    \n",
    "    def reset(self, init_pos):\n",
    "        self.current_pos=init_pos\n",
    "        self.done=(self.target==init_pos)\n",
    "        \n",
    "    def step(self, action):\n",
    "        nrows,ncols=self.dimensions\n",
    "        if action==0:\n",
    "            self.current_pos=(self.current_pos[0], min(self.current_pos[1]+1, float(nrows)))\n",
    "        if action==1:\n",
    "            self.current_pos=(min(self.current_pos[0]+1, float(ncols)), self.current_pos[1])\n",
    "        if action==2:\n",
    "            self.current_pos=(self.current_pos[0], max(self.current_pos[1]-1, 0.0))\n",
    "        if action==3:\n",
    "            self.current_pos=(max(self.current_pos[0]-1, 0.0), self.current_pos[1])     \n",
    "        observation=self.observe()\n",
    "        terminated=False     \n",
    "        if self.current_pos==self.target:\n",
    "            reward=1.0\n",
    "            terminated=True\n",
    "        elif self.current_pos==self.penalty:\n",
    "            reward=-2.0\n",
    "        else:\n",
    "            reward=0.0       \n",
    "        return observation, reward, terminated\n",
    "    \n",
    "    def show(self):\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.title(\"Gridworld\")\n",
    "        plt.imshow(np.zeros((nrows,ncols)),cmap=\"Greys\")\n",
    "        ax=plt.gca()\n",
    "        ax.set_xticks(np.arange(-.5, nrows, 1), minor=True)\n",
    "        ax.set_yticks(np.arange(-.5, ncols, 1), minor=True)\n",
    "        ax.grid(which='minor', color='k', linestyle='-', linewidth=1)\n",
    "        ax.tick_params(which='minor', bottom=False, left=False)\n",
    "        plt.scatter(self.target[0], self.target[1], label='target', color='y', s=100)\n",
    "        plt.scatter(self.current_pos[0], self.current_pos[1], label='current', color='red', s=100)\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm2L5CvPGZEs"
   },
   "source": [
    "We can create and display our environment, in which the reward is fixed, while our initial position is randomized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4VXJQOVGZEs"
   },
   "outputs": [],
   "source": [
    "nrows=6\n",
    "ncols=7\n",
    "gamma=0.9\n",
    "target_pos=(3.0, 1.0)\n",
    "env=Gridworld(nrows, ncols, target_pos=target_pos, init_pos=(float(np.random.randint(ncols)), float(np.random.randint(nrows))))\n",
    "\n",
    "env.show()\n",
    "print(env.observe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4gF6SeMGZEt"
   },
   "source": [
    "This task should be very easy to solve (we wouldn't even really need DL), so we can use a very simple MLP architecture as a Q-Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNuM3eixGZEu"
   },
   "outputs": [],
   "source": [
    "class QNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1=torch.nn.Linear(2,128)\n",
    "        self.layer2=torch.nn.Linear(128,64)\n",
    "        self.layer3=torch.nn.Linear(64,4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=torch.relu(self.layer1(x))\n",
    "        x=torch.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Js9ppDxyGZEu"
   },
   "source": [
    "The following two functions will be used to visualize the policy we learn: since we have a small and fixed number of states ($nrows\\times ncols$) and actions (4), we can query our Q-Network in each of these state-action pairs to obtain the Q value $Q(s,a)$.\n",
    "\n",
    "Once we have computed values for $Q(s,a)$, we can derive a greedy policy by taking the $\\arg\\max$ over actions in each state:\n",
    "$$\n",
    "\\pi(a|s)=\\arg\\max_a Q(s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_IjVQGFGZEu"
   },
   "outputs": [],
   "source": [
    "def compute_q(net):\n",
    "    Q=np.zeros((4, nrows, ncols))\n",
    "    for x in range(ncols):\n",
    "        for y in range(nrows):\n",
    "            for a in range(4):\n",
    "                Q[a, y, x]=net(torch.tensor([x,y], dtype=torch.float))[a].item()\n",
    "    return Q\n",
    "\n",
    "def plot_policy(model, target, penalty=None):\n",
    "    cmap = colors.ListedColormap(['white', 'red', 'green', 'blue'])\n",
    "    bounds=[0,1,2,3,4]\n",
    "    norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "    policy=np.argmax(compute_q(model), axis=0)\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.title(\"Learned Policy\")\n",
    "    img=plt.imshow(policy,cmap=cmap, norm=norm)\n",
    "    plt.scatter(target[0], target[1], label='target', color='y', s=200)\n",
    "    if penalty is not None:\n",
    "        plt.scatter(penalty[0], penalty[1], label='penalty', color='k', s=200)\n",
    "    cbar=plt.colorbar(img, boundaries=bounds)\n",
    "    cbar.set_ticks([0.5,1.5,2.5,3.5])\n",
    "    cbar.ax.set_yticklabels(['Down','Right', 'Up', 'Left'])\n",
    "    plt.legend(prop={'size': 14})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZblWrmkwGZEu"
   },
   "source": [
    "Now we can train our network using Q-Learning.\n",
    "\n",
    "Pay attention to the fact that our exploration rate $\\epsilon$ and learning rate decrease with training time and to the fact that we need to make the target constant w.r.t. the parameters: we obtain this by calling the `.detach()` method of `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_RXN7grGZEu"
   },
   "outputs": [],
   "source": [
    "model=QNet()\n",
    "\n",
    "eps=0.9\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=.995)\n",
    "loss=torch.nn.MSELoss()\n",
    "n_episodes=2000\n",
    "\n",
    "durations=[]\n",
    "learning_rates=[]\n",
    "epsilons=[]\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    eps=eps*0.995\n",
    "    env.reset((float(np.random.randint(ncols)), float(np.random.randint(nrows))))\n",
    "    t=0\n",
    "    done=env.done\n",
    "    while (t<100 and not done):\n",
    "        t+=1\n",
    "        q=model(torch.tensor(env.observe()))\n",
    "        A_t=int(np.random.randint(4) if np.random.rand()<eps else torch.argmax(q).reshape(1))\n",
    "        observation,reward,done=env.step(A_t)\n",
    "        target=torch.tensor(reward) if done else reward+gamma*torch.max(model(torch.tensor(observation).detach()))\n",
    "        l=loss(target, q[A_t])\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    durations.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-1HNR66GZEv"
   },
   "source": [
    "The network quickly learns a suitable policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISCzkBbjGZEv"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.title(\"Episode duration\")\n",
    "plt.plot(durations, 'o', markersize=2)\n",
    "plt.plot(np.convolve(durations, np.ones(10), mode='valid')/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNZrZs4YGZEv"
   },
   "outputs": [],
   "source": [
    "plot_policy(model, target_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdwLcnXwGZEv"
   },
   "source": [
    "Now let's try to see what happens if we add a penalty cell: the agent should learn a policy that avoids passing through it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXkqu4_fGZEv"
   },
   "outputs": [],
   "source": [
    "model=QNet()\n",
    "penalty_pos=(1.0, 4.0)\n",
    "env=Gridworld(nrows, ncols, target_pos=target_pos, init_pos=(float(np.random.randint(ncols)), float(np.random.randint(nrows))), penalty_pos=penalty_pos)\n",
    "\n",
    "eps=0.9\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=.995)\n",
    "loss=torch.nn.MSELoss()\n",
    "n_episodes=2000\n",
    "\n",
    "durations=[]\n",
    "learning_rates=[]\n",
    "epsilons=[]\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    eps=eps*0.995\n",
    "    env.reset((float(np.random.randint(ncols)), float(np.random.randint(nrows))))\n",
    "    t=0\n",
    "    done=env.done\n",
    "    while (t<100 and not done):\n",
    "        t+=1\n",
    "        q=model(torch.tensor(env.observe()))\n",
    "        A_t=int(np.random.randint(4) if np.random.rand()<eps else torch.argmax(q).reshape(1))\n",
    "        observation,reward,done=env.step(A_t)\n",
    "        target=torch.tensor(reward) if done else reward+gamma*torch.max(model(torch.tensor(observation).detach()))\n",
    "        l=loss(target, q[A_t])\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    durations.append(t)\n",
    "    epsilons.append(eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SfVDUlVGZEw"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.title(\"Episode duration\")\n",
    "plt.plot(durations, 'o', markersize=2)\n",
    "plt.plot(np.convolve(durations, np.ones(10), mode='valid')/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGr4bOfIGZEw"
   },
   "outputs": [],
   "source": [
    "plot_policy(model, target_pos, penalty_pos)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
