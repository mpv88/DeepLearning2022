{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SN8Et5No4K1z"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lorenzobasile/DeepLearning2022/blob/main/5_autoencoder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wq2yvzF0Ykzk"
   },
   "source": [
    "# Lab 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkcNSlCSYkzl"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "%cd drive/MyDrive/DeepLearning2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LY2rtd_sYkzn"
   },
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iQXmoX1Ykzn"
   },
   "source": [
    "Autoencoders are neural models that aim at **reconstructing** data. An autoencoder is made of an encoder part, that maps the input in a latent (or hidden) representation and a decoder, which maps this representation back in the initial space.\n",
    "\n",
    "This is not a requirement, but in most cases the hidden space is much lower-dimensional than the input (and output) space, meaning that the autoencoder is performing **dimensionality reduction**, learning a low-dimensional code that represents a substantial fraction of the variability of the input data. If trained with MSE Loss and without nonlinearities, it can be shown to perform equivalently to Principal Component Analysis (PCA).\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUD5ySR1-WCg"
   },
   "source": [
    "## Handling RGB images and hardware acceleration\n",
    "\n",
    "We will see an application example of an autoencoder on colored images. The dataset we will use is CIFAR10 (available on `torchvision`). CIFAR10 is one of the most famous benchmarks for neural networks, and in a sense it is the colored equivalent of MNIST, as it contains 10 classes of 32x32 RGB images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAyLAOm8Ykzo"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NQ7Cl2l-sWY"
   },
   "source": [
    "We define a function to show some images and their reconstructions. Please pay attention to the transposition step. Our batch of data will have shape [batch, channels, height, width], but to use matplotlib functions we need to convert it to [batch, height, width, channels]. We can do that by using the `transpose` function, with proper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2Ih-UJMYkzn"
   },
   "outputs": [],
   "source": [
    "def display_images(input, output):\n",
    "    if input is not None:\n",
    "        input_pics = input.data.cpu().numpy().transpose((0,2,3,1))\n",
    "        plt.figure(figsize=(18, 4))\n",
    "        for i in range(4):\n",
    "            plt.subplot(1,4,i+1)\n",
    "            plt.imshow(input_pics[i])\n",
    "    plt.figure(figsize=(18, 4))\n",
    "    output_pics = output.data.cpu().numpy().transpose((0,2,3,1))\n",
    "    for i in range(4):\n",
    "        plt.subplot(1,4,i+1)\n",
    "        plt.imshow(output_pics[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1ePpIkiAAvN"
   },
   "source": [
    "We are using a GPU hardware accelerator, so we need to inform torch of this resource by defining a device variable. We will later load our model and our data on the GPU to exploit its capabilities. The following line guarantees that if the GPU is not available the code below can still work without any issue on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIQhpmBkYkzo"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjMNIiWYB2iW"
   },
   "source": [
    "## A convolutional autoencoder architecture\n",
    "\n",
    "Since we are dealing with image data, it makes sense to exploit the impicit bias introduced by convolutions, making our Autoencoder a Convolutional Autoencoder.\n",
    "\n",
    "For the encoder part, this simply translates into having a CNN (similar to a classifier, such as the one we saw during the last lecture), that given an image returns a hidden representation of size $d$.\n",
    "\n",
    "Then, this representation is fed into the decoder network, which employs transpose convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCTsdofNCyAF"
   },
   "source": [
    "### Transpose Convolutions\n",
    "\n",
    "Transpose Convolutions follow a working principle that is very similar to \"normal\" Convolutions, but they are used to enlarge an image instead of reducing its size.\n",
    "\n",
    "The parameters to construct a Transpose Conv layer are exactly the same:\n",
    "```\n",
    "ConvTranspose2d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int]] = 0)\n",
    "```\n",
    "However, note that the meaning of some of these parameters may be counterintuitive, the whole process flows like this:\n",
    "\n",
    "- We start from an image;\n",
    "- We space the pixels apart so that the step between two pixels is equal to the `stride`;\n",
    "- We pad the image with as many rows and columns of 0s as we can, while making sure that the kernel always contains at least one \"meaningful\" pixel;\n",
    "- We remove external rows and columns according to the `padding` parameter: pay attention! now we use this parameter to **remove** lines, not to add them;\n",
    "- Finally we move our filter through the image using a step size of 1.\n",
    "\n",
    "[Example with `stride=2`, `kernel_size=3`, `padding=0`.](https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_strides_transposed.gif)\n",
    "\n",
    "[Example with `stride=2`, `kernel_size=3`, `padding=1`.](https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/padding_strides_transposed.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHknv_RuYkzo"
   },
   "outputs": [],
   "source": [
    "d = 50\n",
    "\n",
    "class AE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5, stride=1, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(128*7*7, d)\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d, 128*8*8),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Unflatten(1, torch.Size([128, 8, 8])),\n",
    "            torch.nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=2, stride=2),\n",
    "            torch.nn.Sigmoid()\n",
    "            \n",
    "        )    \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "        return out\n",
    "\n",
    "model = AE().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OjgxSR773fa"
   },
   "source": [
    "Training an Autoencoder consists in performing regression of the output on the input, so we employ MSE Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oht7b8y2Ykzp"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-awD0RDQo3t"
   },
   "source": [
    "We run only 5 epochs just to show that everything works, to obtain better performances you can try to extend the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYsiYMlBYkzp"
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x,y in iter(trainloader):\n",
    "        x=x.to(device)\n",
    "        x_hat=model(x)\n",
    "        l=loss(x_hat,x)\n",
    "        train_loss+=l.item()\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Epoch \"+str(epoch+1)+\" Training loss:\"+str(train_loss / (len(trainloader.dataset))))\n",
    "        \n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_loss=0 \n",
    "    for x,y in iter(testloader):\n",
    "            x=x.to(device)\n",
    "            x_hat=model(x)\n",
    "            l=loss(x_hat,x)\n",
    "            test_loss+=l.item()\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    print(\"Test set loss:\"+str(test_loss))\n",
    "    display_images(x, x_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3fCdvmGms18"
   },
   "source": [
    "# Second assignment (deadline 11 December)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuJZRcSFjbGx"
   },
   "source": [
    "\n",
    "- 1. Compute the Intrinsic Dimension of KMNIST dataset using [TwoNN](https://www.nature.com/articles/s41598-017-11873-y). There is no need to implement TwoNN from scratch: you can use the implementation in the [`dadapy` package](https://dadapy.readthedocs.io/en/latest/jupyter_example_3.html), which you can install via `pip install dadapy`.\n",
    "- 2. Build a Convolutional Autoencoder to reduce the dimensionality of KMNIST to a value $d$ that is compatible with the ID estimate you found in step 1. Once you have trained the model (until you notice some stability in the loss, likely not before 20 epochs of training), plot some examples of reconstructed images to qualitatively assess how well the model is performing. For this step, you can use the following setup (or modify it if you prefer):\n",
    "\n",
    "\n",
    "```\n",
    "Encoder:\n",
    "\tConvolutional layer with 32 filters, kernel size=3, stride=1, padding=0\n",
    "    Max pooling layer with kernel size=2\n",
    "\tConvolutional layer with 64 filters, kernel size=3, stride=2, padding=1\n",
    "    Max pooling layer with kernel size=2, stride=1\n",
    "\tFlattening layer\n",
    "\tLinear layer with input size=6x6x64, output size=d\n",
    "\tReLU activations wherever needed\n",
    "\n",
    "Decoder:\n",
    "\tLinear layer with input size=d, output size=64x8x8\n",
    "\tUnflattening layer to obtain [64x8x8] data\n",
    "\tTranspose Convolution with 32 filters, kernel size=2, stride=2, padding=1\n",
    "\tTranspose Convolution with 1 filter, kernel size=2, stride=2, padding=0\n",
    "\tReLU activations wherever needed\n",
    "```\n",
    "\n",
    "- 3. If the encoder is performing sufficiently well, you should be able to perform almost any task of your choice on the original data by only looking at the internal hidden representations that it extracts. To see if this is the case, build a new model made of the encoder you have already trained (freezed) and a classification head (a trainable fully connected layer with `in_features=d` and `ou_features=10`). Train this new model for classification and assess the final accuracy. Don't expect perfect accuracy: anything substantially higher than random classification is fine.\n",
    "\n",
    "You can send your work as a jupyter notebook in any format you prefer (`ipynb`, `pdf` or `html`) to lore.basile@outlook.com by 23.59, 11/12/2022. Please name the file as `NameSurname.<format>`.\n",
    "\n",
    "As always, please do not hesitate to reach out if there are doubts or difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note after lab (**VERY** optional part of the assignment)\n",
    "\n",
    "As pointed out by prof. Ansuini during the lab, it may be interesting to visualize the representations that your Autoencoder learns while training. Since your hidden dimension $d$ will likely be much higher than 2 or 3, it would be impossible to simply plot the representations and see where they end up being.\n",
    "\n",
    "To visualize them, you could use another dimensionality reduction technique on top of your Encoder, such as t-SNE, to further reduce your $d$ features to 2 or 3, just like what is shown for example in this nice [github repo](https://github.com/ncampost/vis-autoencoder-tsne) and in this great [lecture](https://atcold.github.io/NYU-DLSP21/en/week09/09-3/) by Alfredo Canziani, which is also a good reference to study Autoencoders in general.\n",
    "\n",
    "What is shown in this repo (and hopefully this may be visible also in this assignment, on KMNIST) is that after the Autoencoder is trained, it learns to somehow map the images belonging to the same class in a cluster, meaning that it could really capture features that are meaningful to a certain class. In a sense, such a finding would also be reassuring when moving to the classification task (clustered representations are much more likely to be easily separable).\n",
    "\n",
    "**Note**: in the lecture by Alfredo Canziani the t-SNE plot is in the section on Variational Autoencoders, a topic that we will cover in the next lectures, but in principle this kind of clustering behaviour may appear also in normal Autoencoders like the ones we are working on.\n",
    "\n",
    "If you want, you are welcome to try this kind of approach in your work. However, please note that this step is not a requirement."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
