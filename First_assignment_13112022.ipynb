{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mpv88/DeepLearning2022/blob/main/First_assignment_13112022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RY64HwBgDLg"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lorenzobasile/DeepLearning2022/blob/main/gradient_descent.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#First assignment - 11/2022 - *mattia pividori (s284690)*\n",
        "---"
      ],
      "metadata": {
        "id": "TxnN0O8w345d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuJZRcSFjbGx"
      },
      "source": [
        "\n",
        "- 1. Read carefully the paper [Learning representations by back-propagating errors](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)\n",
        "- 2. Reproduce in PyTorch (or any other DL library you like) experiment 1. Try to be as close as possible to the original protocol regarding network architecture, activation function, training algorithm and parameter initialization.\n",
        "    - Inspect the weights you obtained and check if they provide a solution to the problem\n",
        "    - Compare the solution to the solution reported in the paper\n",
        "- 3. Write a small report (1 page) about your experiment and what you\n",
        "learned about that. The report should be a jupyter notebook with text\n",
        "cells that describe the non-trivial parts of your work."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we import the required Python libraries. we also fix a seed for reproducibility of our results."
      ],
      "metadata": {
        "id": "CZxsviyHJKZO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YgxcemAAYNg9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c7288c26-99fe-4cc4-faed-871c1fcb2f18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f564105d2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create a synthetic dataset, in line with what described into \"Learning representations by back-propagating errors\" by Rumelhart et aliis (1986).\n",
        "To this purpose we define three auxiliary functions: \n",
        " \n",
        "\n",
        "*   \"to_binary\" serves the purpose of first, converting an integer number to its\n",
        "binary counterparty, then extending the binary representation of the number to a desired number of chars (corresponding to the desired vectors' length); finally, if the binary representation itself does not fill the required length already, the remaining (empty) places are padded with zeros. \n",
        "\n",
        "\n",
        "*   \"check_symmetry\" aims at verifying if a given vector is symmetric or not by means of three conditions: if the number of columns of a given vector is not even, then vector is not symmetrical (print of 0); if instead is even, we check elementwise if the pairs 'first element-last element' -from vector's start up to midpoint- contain two identical elements, if it is the case we have symmetry (print of 1), otherwise we fall again into the asymmetrical case (print of 0).\n",
        "\n",
        "*   \"create_dataset\" is a functions which makes use of the previous helper functions to easily create two arrays: the X which contains our dataset with synthetic observations, and y which constitutes the ground truth. \n",
        "\n",
        "Given that we opt for a (64-row X 6-column) dataset, where each vector is made of binary chars (0 or 1) -to be in line with what proposed in the original paper-, we verify empirically that in this space we end up with a quite unbalanced dataset including just 8 symmetric observations.\n"
      ],
      "metadata": {
        "id": "su2FiHtPJD1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_binary(input_number, vector_lenght):\n",
        "  '''transforms decimals to binaries and leading pad'''\n",
        "  return [int(_) for _ in '{0:b}'.format(input_number).zfill(vector_lenght)]\n",
        "\n",
        "def check_symmetry(input_vector):\n",
        "  '''returns 1 if a vector is symmetrical, 0 otherwise'''\n",
        "  if len(input_vector)%2 != 0: return float(0)\n",
        "  else:\n",
        "    for i in range(len(input_vector)//2):\n",
        "      if input_vector[i] != input_vector[-(1+i)]: \n",
        "        return float(0)\n",
        "    return float(1)\n",
        "\n",
        "def create_dataset(size):\n",
        "  '''generates dataset (X) and ground truth (y)'''\n",
        "  X = torch.tensor([to_binary(_, size) for _ in range(2**size)], dtype=torch.float32, requires_grad=True)\n",
        "  y = torch.tensor([check_symmetry(X[_, :]) for _ in range(2**size)])\n",
        "  return X, y\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "# sample dataset creation\n",
        "vector_size = 6 # see fig. [1] paper\n",
        "X, y = create_dataset(vector_size) # see fig. [1] paper\n",
        "\n",
        "# check total number of symmetric obs within the dataset and which are them:\n",
        "print(torch.sum(y)) # result is 8\n",
        "[print(X[_],y[_]) for _ in range(2**vector_size) if y[_].item()==1]"
      ],
      "metadata": {
        "id": "LtYU3towRrAu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cdb3db0e-254b-4c3f-db8e-ffac21c77331"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8.)\n",
            "tensor([0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>) tensor(1.)\n",
            "tensor([0., 0., 1., 1., 0., 0.], grad_fn=<SelectBackward0>) tensor(1.)\n",
            "tensor([0., 1., 0., 0., 1., 0.], grad_fn=<SelectBackward0>) tensor(1.)\n",
            "tensor([0., 1., 1., 1., 1., 0.], grad_fn=<SelectBackward0>) tensor(1.)\n",
            "tensor([1., 0., 0., 0., 0., 1.], grad_fn=<SelectBackward0>) tensor(1.)\n",
            "tensor([1., 0., 1., 1., 0., 1.], grad_fn=<SelectBackward0>) tensor(1.)\n",
            "tensor([1., 1., 0., 0., 1., 1.], grad_fn=<SelectBackward0>) tensor(1.)\n",
            "tensor([1., 1., 1., 1., 1., 1.], grad_fn=<SelectBackward0>) tensor(1.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None, None, None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step we set up the parameters for our simple Multi Layer Perceptron, in line with what is our understanding of the original paper.\n",
        "The input layer will get a size of 6 (in line with defined input vectors), while the single hidden layer is going to be composed of two hidden units.\n",
        "Finally, being the problem at hand a symmetry detection problem, we can imagine it as a binary classification problem: so it is sufficient for the output layer to have just 1-dimension (i.e. 1 will indicate symmetry, 0 asymmetry).\n",
        "In line with what presented in the paper (see formula [2]), the activation function of choice for our MLP will be a logistic function (a sigmoid curve).\n",
        "We also enable the opportunity to have a bias in our MLP, as explicitly cited in Fig.1 of the paper and in the results of the experiment."
      ],
      "metadata": {
        "id": "nXkYAg2mso23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup MLP specifications\n",
        "\n",
        "n_input = X.size()[1]       # input size corresponds to the size of 1D vectors (1x6)\n",
        "n_hidden_1 = 2              # hidden nodes\n",
        "n_output = 1                # output nodes for binary classifier\n",
        "bias_dummy = True           # bias enabled\n",
        "activation_function = torch.nn.Sigmoid() # see formula [2] paper"
      ],
      "metadata": {
        "id": "_wNX8ff1oWPx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following step we simply encode the information and the parameters defined at previous step, inheriting from a pythorch.module class and thus defining our deep-learning \"binary classifier\".\n",
        "Notice the fact that, following the instructions of Fig.1 in the paper, we have tried to replicate the fact that \"[...] initial weights were random and were uniformly distributed between -0.3 and 0.3\". \n",
        "Finally, as for us it was not clear in the paper if the same operation was applied or not to bias terms, we opted for *not* applying it, anyway we tried to run our model even applying it without significant differences in the results.\n",
        "At the end of the cell you may find a useful print with a recap of the newly created and initialised model, and a sketch of its structure."
      ],
      "metadata": {
        "id": "NYtPs4ivwA8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup Multi-Layer Perceptron (MLP) class\n",
        "\n",
        "class binary_classifier(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_1 = torch.nn.Linear(in_features=n_input, out_features=n_hidden_1, bias=bias_dummy) # first layer: from input 1D vector (1x6) to hidden layer of made of 2 nodes\n",
        "    self.layer_2 = torch.nn.Linear(in_features=n_hidden_1, out_features=n_output, bias=bias_dummy) # second layer: from 2 node hidden layer to output (1/0 binary classification problem)\n",
        "    self.activation = activation_function\n",
        "    torch.nn.init.uniform_(self.layer_1.weight, -0.3, 0.3)\n",
        "    torch.nn.init.uniform_(self.layer_2.weight, -0.3, 0.3)\n",
        "    #torch.nn.init.uniform_(self.layer_1.bias,-0.3,0.3)\n",
        "    #torch.nn.init.uniform_(self.layer_2.bias,-0.3,0.3)\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = self.activation(self.layer_1(X))\n",
        "    X = self.activation(self.layer_2(X))\n",
        "    return X\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "# initialize model\n",
        "model = binary_classifier()\n",
        "# first check of initialized mlp parameters\n",
        "print(model)\n",
        "for name,param in model.named_parameters():\n",
        "    print(name, param.data)"
      ],
      "metadata": {
        "id": "_Z-cqHsG55-8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c2a07357-f6a8-4eaf-b2b4-130c2fc28bb0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "binary_classifier(\n",
            "  (layer_1): Linear(in_features=6, out_features=2, bias=True)\n",
            "  (layer_2): Linear(in_features=2, out_features=1, bias=True)\n",
            "  (activation): Sigmoid()\n",
            ")\n",
            "layer_1.weight tensor([[ 0.2303,  0.1850,  0.1517,  0.2393,  0.1103,  0.1595],\n",
            "        [ 0.2489, -0.0604, -0.2340, -0.1475, -0.0400, -0.0330]])\n",
            "layer_1.bias tensor([ 0.1159, -0.1653])\n",
            "layer_2.weight tensor([[-0.0020,  0.1719]])\n",
            "layer_2.bias tensor([-0.6145])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell we define the last two elements necessary before training our model:\n",
        "\n",
        "*   the loss function: defined below are two alternatives ready to be used on our MLP; the first is directly coming from formula [3] in paper and defines \"the total error in the performance of the network [...] by comparing actual and desired output vectors for every case\"; the second is a Binary Cross Entropy loss (details at https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) a de facto standard for evaluating the performance of binary classification tasks.\n",
        "*   the optimizer: the choice is for the stochastic gradient descent (SGD) optimizer (details at https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) where we specify some options such as the learning rate and the momentum. In fact, as per formula [9] in paper and the details outlined in Fig. 1 we understand that the learning rate should be equal to the ε parameter, fixed to 0.1, while the momentum is set equal to the α parameter (0.9), which enables an \"acceleration method in which the current gradient is used to modfiy the velocity of the point instead of its position\".\n",
        "The last parameter is the total number of epochs (i.e. \"sweeps\" in the paper\") for training the model, which is set here to 1425.\n",
        "\n"
      ],
      "metadata": {
        "id": "IDUA65IEzE-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss functions: 2 alternatives\n",
        "\n",
        "def total_error(y_predicted, y_truth):\n",
        "            return 0.5 * torch.sum((y_predicted-y_truth) **2) # see formula [3] paper\n",
        "te_loss = total_error\n",
        "\n",
        "bce_loss = torch.nn.BCELoss() # used Binary Cross Entropy loss\n",
        "\n",
        "\n",
        "# optimizer: stochastic gradient descent (optionally with momentum)\n",
        "\n",
        "num_epochs = 1425  # i.e. sweeps see fig. [1] paper\n",
        "epsilon = 0.1 # i.e. ε of equation [9], see fig. [1] paper\n",
        "alpha = 0.9 # i.e. # i.e. α of equation [9], see fig. [1] paper\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=epsilon, momentum=alpha) # gradient descent with momentum"
      ],
      "metadata": {
        "id": "b6UZmNUy4lY6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell we define the function enabling the SGD training by means of the iterative process already explained in the previouses labs, which encopasses some steps that we are going to repeat for a number of times equal to the stated epochs. In detail, the sub-steps include first a loop over data, then a first forward pass which leads to a first guess of the output, then the computation of the loss follows, the past gradients are erased as convenient within the Pytorch framework; thus the gradients are computed within a backward pass steps which ends up with the updating of the initial parameters."
      ],
      "metadata": {
        "id": "ozbvW3FSzGTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X, y, model, loss_fn, optimizer, num_epochs):\n",
        "  model.train()\n",
        "  train_loss = []\n",
        "  for epoch in range(num_epochs):\n",
        "    y_predicted = model(X) # forward pass, get model's output\n",
        "    loss = te_loss(y_predicted, y.unsqueeze(1)) # calculate the loss \n",
        "    optimizer.zero_grad() # gradients' reset (avoid loop cumulation)\n",
        "    loss.backward() # backward propagation on current loss\n",
        "    optimizer.step() # update params\n",
        "    print(\"Loss in iteration: \"+str(epoch)+\" is: \"+str(loss.item()))\n",
        "    train_loss.append(loss.item())\n",
        "  return train_loss"
      ],
      "metadata": {
        "id": "7W7__65QmNjz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the short following step we finally train our MLP model, and check the resulting parameters Θ for our model (weights and biases), together with the trend of the loss function while increasing model's iterations."
      ],
      "metadata": {
        "id": "x9PLQgjA9gv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model run\n",
        "train_loss = train_model(X, y, model, te_loss, optimizer, num_epochs)\n",
        "#train_model(X, y, model, te_loss, optimizer, num_epochs)\n",
        "\n",
        "# final check of mlp params\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.data)"
      ],
      "metadata": {
        "id": "cpXVrtXUwbkx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2b2b0cbe-b5b2-4833-a6aa-ec380b3d21cb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss in iteration: 0 is: 2.80287504196167\n",
            "Loss in iteration: 1 is: 2.8028721809387207\n",
            "Loss in iteration: 2 is: 2.8028690814971924\n",
            "Loss in iteration: 3 is: 2.802866220474243\n",
            "Loss in iteration: 4 is: 2.802863121032715\n",
            "Loss in iteration: 5 is: 2.8028602600097656\n",
            "Loss in iteration: 6 is: 2.802856683731079\n",
            "Loss in iteration: 7 is: 2.802853584289551\n",
            "Loss in iteration: 8 is: 2.8028509616851807\n",
            "Loss in iteration: 9 is: 2.8028476238250732\n",
            "Loss in iteration: 10 is: 2.802845001220703\n",
            "Loss in iteration: 11 is: 2.802841901779175\n",
            "Loss in iteration: 12 is: 2.8028388023376465\n",
            "Loss in iteration: 13 is: 2.802835464477539\n",
            "Loss in iteration: 14 is: 2.80283260345459\n",
            "Loss in iteration: 15 is: 2.8028297424316406\n",
            "Loss in iteration: 16 is: 2.8028268814086914\n",
            "Loss in iteration: 17 is: 2.802823781967163\n",
            "Loss in iteration: 18 is: 2.802820920944214\n",
            "Loss in iteration: 19 is: 2.8028173446655273\n",
            "Loss in iteration: 20 is: 2.8028147220611572\n",
            "Loss in iteration: 21 is: 2.80281138420105\n",
            "Loss in iteration: 22 is: 2.802809000015259\n",
            "Loss in iteration: 23 is: 2.8028054237365723\n",
            "Loss in iteration: 24 is: 2.802802562713623\n",
            "Loss in iteration: 25 is: 2.8027992248535156\n",
            "Loss in iteration: 26 is: 2.8027963638305664\n",
            "Loss in iteration: 27 is: 2.802793502807617\n",
            "Loss in iteration: 28 is: 2.802790641784668\n",
            "Loss in iteration: 29 is: 2.8027877807617188\n",
            "Loss in iteration: 30 is: 2.8027842044830322\n",
            "Loss in iteration: 31 is: 2.802781581878662\n",
            "Loss in iteration: 32 is: 2.802778482437134\n",
            "Loss in iteration: 33 is: 2.8027756214141846\n",
            "Loss in iteration: 34 is: 2.8027725219726562\n",
            "Loss in iteration: 35 is: 2.802769422531128\n",
            "Loss in iteration: 36 is: 2.8027665615081787\n",
            "Loss in iteration: 37 is: 2.8027634620666504\n",
            "Loss in iteration: 38 is: 2.802760601043701\n",
            "Loss in iteration: 39 is: 2.80275821685791\n",
            "Loss in iteration: 40 is: 2.8027544021606445\n",
            "Loss in iteration: 41 is: 2.8027515411376953\n",
            "Loss in iteration: 42 is: 2.802748680114746\n",
            "Loss in iteration: 43 is: 2.8027453422546387\n",
            "Loss in iteration: 44 is: 2.8027429580688477\n",
            "Loss in iteration: 45 is: 2.8027398586273193\n",
            "Loss in iteration: 46 is: 2.802737236022949\n",
            "Loss in iteration: 47 is: 2.802733898162842\n",
            "Loss in iteration: 48 is: 2.8027307987213135\n",
            "Loss in iteration: 49 is: 2.802727699279785\n",
            "Loss in iteration: 50 is: 2.802725076675415\n",
            "Loss in iteration: 51 is: 2.8027219772338867\n",
            "Loss in iteration: 52 is: 2.8027186393737793\n",
            "Loss in iteration: 53 is: 2.802716016769409\n",
            "Loss in iteration: 54 is: 2.8027124404907227\n",
            "Loss in iteration: 55 is: 2.8027100563049316\n",
            "Loss in iteration: 56 is: 2.8027071952819824\n",
            "Loss in iteration: 57 is: 2.802703857421875\n",
            "Loss in iteration: 58 is: 2.802700996398926\n",
            "Loss in iteration: 59 is: 2.8026978969573975\n",
            "Loss in iteration: 60 is: 2.8026952743530273\n",
            "Loss in iteration: 61 is: 2.802692174911499\n",
            "Loss in iteration: 62 is: 2.802689552307129\n",
            "Loss in iteration: 63 is: 2.8026864528656006\n",
            "Loss in iteration: 64 is: 2.8026838302612305\n",
            "Loss in iteration: 65 is: 2.802680492401123\n",
            "Loss in iteration: 66 is: 2.8026769161224365\n",
            "Loss in iteration: 67 is: 2.8026745319366455\n",
            "Loss in iteration: 68 is: 2.8026716709136963\n",
            "Loss in iteration: 69 is: 2.802668571472168\n",
            "Loss in iteration: 70 is: 2.8026657104492188\n",
            "Loss in iteration: 71 is: 2.8026623725891113\n",
            "Loss in iteration: 72 is: 2.802659511566162\n",
            "Loss in iteration: 73 is: 2.802656650543213\n",
            "Loss in iteration: 74 is: 2.8026537895202637\n",
            "Loss in iteration: 75 is: 2.8026506900787354\n",
            "Loss in iteration: 76 is: 2.802647829055786\n",
            "Loss in iteration: 77 is: 2.802644729614258\n",
            "Loss in iteration: 78 is: 2.802642345428467\n",
            "Loss in iteration: 79 is: 2.8026392459869385\n",
            "Loss in iteration: 80 is: 2.802635908126831\n",
            "Loss in iteration: 81 is: 2.8026328086853027\n",
            "Loss in iteration: 82 is: 2.8026301860809326\n",
            "Loss in iteration: 83 is: 2.8026270866394043\n",
            "Loss in iteration: 84 is: 2.802624464035034\n",
            "Loss in iteration: 85 is: 2.802621603012085\n",
            "Loss in iteration: 86 is: 2.8026185035705566\n",
            "Loss in iteration: 87 is: 2.8026158809661865\n",
            "Loss in iteration: 88 is: 2.8026130199432373\n",
            "Loss in iteration: 89 is: 2.802609920501709\n",
            "Loss in iteration: 90 is: 2.802607297897339\n",
            "Loss in iteration: 91 is: 2.8026039600372314\n",
            "Loss in iteration: 92 is: 2.8026013374328613\n",
            "Loss in iteration: 93 is: 2.802598714828491\n",
            "Loss in iteration: 94 is: 2.802595376968384\n",
            "Loss in iteration: 95 is: 2.8025922775268555\n",
            "Loss in iteration: 96 is: 2.8025894165039062\n",
            "Loss in iteration: 97 is: 2.802586555480957\n",
            "Loss in iteration: 98 is: 2.802583694458008\n",
            "Loss in iteration: 99 is: 2.8025810718536377\n",
            "Loss in iteration: 100 is: 2.8025779724121094\n",
            "Loss in iteration: 101 is: 2.802574634552002\n",
            "Loss in iteration: 102 is: 2.8025717735290527\n",
            "Loss in iteration: 103 is: 2.8025689125061035\n",
            "Loss in iteration: 104 is: 2.8025660514831543\n",
            "Loss in iteration: 105 is: 2.802563190460205\n",
            "Loss in iteration: 106 is: 2.802560329437256\n",
            "Loss in iteration: 107 is: 2.802557945251465\n",
            "Loss in iteration: 108 is: 2.8025546073913574\n",
            "Loss in iteration: 109 is: 2.802551746368408\n",
            "Loss in iteration: 110 is: 2.802549362182617\n",
            "Loss in iteration: 111 is: 2.8025460243225098\n",
            "Loss in iteration: 112 is: 2.8025429248809814\n",
            "Loss in iteration: 113 is: 2.802539825439453\n",
            "Loss in iteration: 114 is: 2.802536725997925\n",
            "Loss in iteration: 115 is: 2.802534580230713\n",
            "Loss in iteration: 116 is: 2.8025312423706055\n",
            "Loss in iteration: 117 is: 2.8025283813476562\n",
            "Loss in iteration: 118 is: 2.8025259971618652\n",
            "Loss in iteration: 119 is: 2.802522897720337\n",
            "Loss in iteration: 120 is: 2.8025197982788086\n",
            "Loss in iteration: 121 is: 2.8025171756744385\n",
            "Loss in iteration: 122 is: 2.802513837814331\n",
            "Loss in iteration: 123 is: 2.802511215209961\n",
            "Loss in iteration: 124 is: 2.8025081157684326\n",
            "Loss in iteration: 125 is: 2.8025054931640625\n",
            "Loss in iteration: 126 is: 2.8025026321411133\n",
            "Loss in iteration: 127 is: 2.802499294281006\n",
            "Loss in iteration: 128 is: 2.802497148513794\n",
            "Loss in iteration: 129 is: 2.8024942874908447\n",
            "Loss in iteration: 130 is: 2.8024911880493164\n",
            "Loss in iteration: 131 is: 2.802488088607788\n",
            "Loss in iteration: 132 is: 2.802485704421997\n",
            "Loss in iteration: 133 is: 2.802482843399048\n",
            "Loss in iteration: 134 is: 2.8024797439575195\n",
            "Loss in iteration: 135 is: 2.802476167678833\n",
            "Loss in iteration: 136 is: 2.802473783493042\n",
            "Loss in iteration: 137 is: 2.802471160888672\n",
            "Loss in iteration: 138 is: 2.8024682998657227\n",
            "Loss in iteration: 139 is: 2.8024654388427734\n",
            "Loss in iteration: 140 is: 2.802462577819824\n",
            "Loss in iteration: 141 is: 2.802459955215454\n",
            "Loss in iteration: 142 is: 2.802456855773926\n",
            "Loss in iteration: 143 is: 2.8024539947509766\n",
            "Loss in iteration: 144 is: 2.8024508953094482\n",
            "Loss in iteration: 145 is: 2.802448272705078\n",
            "Loss in iteration: 146 is: 2.802445411682129\n",
            "Loss in iteration: 147 is: 2.802442789077759\n",
            "Loss in iteration: 148 is: 2.8024396896362305\n",
            "Loss in iteration: 149 is: 2.8024368286132812\n",
            "Loss in iteration: 150 is: 2.802433967590332\n",
            "Loss in iteration: 151 is: 2.8024308681488037\n",
            "Loss in iteration: 152 is: 2.8024282455444336\n",
            "Loss in iteration: 153 is: 2.8024258613586426\n",
            "Loss in iteration: 154 is: 2.802422523498535\n",
            "Loss in iteration: 155 is: 2.802419662475586\n",
            "Loss in iteration: 156 is: 2.802417039871216\n",
            "Loss in iteration: 157 is: 2.8024144172668457\n",
            "Loss in iteration: 158 is: 2.8024113178253174\n",
            "Loss in iteration: 159 is: 2.8024086952209473\n",
            "Loss in iteration: 160 is: 2.802405595779419\n",
            "Loss in iteration: 161 is: 2.8024027347564697\n",
            "Loss in iteration: 162 is: 2.8024001121520996\n",
            "Loss in iteration: 163 is: 2.8023974895477295\n",
            "Loss in iteration: 164 is: 2.802394390106201\n",
            "Loss in iteration: 165 is: 2.802391529083252\n",
            "Loss in iteration: 166 is: 2.8023886680603027\n",
            "Loss in iteration: 167 is: 2.802386522293091\n",
            "Loss in iteration: 168 is: 2.8023834228515625\n",
            "Loss in iteration: 169 is: 2.802380084991455\n",
            "Loss in iteration: 170 is: 2.802377462387085\n",
            "Loss in iteration: 171 is: 2.802374839782715\n",
            "Loss in iteration: 172 is: 2.8023722171783447\n",
            "Loss in iteration: 173 is: 2.802368640899658\n",
            "Loss in iteration: 174 is: 2.802366256713867\n",
            "Loss in iteration: 175 is: 2.802363395690918\n",
            "Loss in iteration: 176 is: 2.8023605346679688\n",
            "Loss in iteration: 177 is: 2.8023581504821777\n",
            "Loss in iteration: 178 is: 2.8023550510406494\n",
            "Loss in iteration: 179 is: 2.8023524284362793\n",
            "Loss in iteration: 180 is: 2.802349090576172\n",
            "Loss in iteration: 181 is: 2.8023462295532227\n",
            "Loss in iteration: 182 is: 2.8023436069488525\n",
            "Loss in iteration: 183 is: 2.8023409843444824\n",
            "Loss in iteration: 184 is: 2.802338123321533\n",
            "Loss in iteration: 185 is: 2.802335500717163\n",
            "Loss in iteration: 186 is: 2.8023321628570557\n",
            "Loss in iteration: 187 is: 2.8023297786712646\n",
            "Loss in iteration: 188 is: 2.8023271560668945\n",
            "Loss in iteration: 189 is: 2.8023242950439453\n",
            "Loss in iteration: 190 is: 2.802321195602417\n",
            "Loss in iteration: 191 is: 2.802318572998047\n",
            "Loss in iteration: 192 is: 2.8023157119750977\n",
            "Loss in iteration: 193 is: 2.8023130893707275\n",
            "Loss in iteration: 194 is: 2.802309989929199\n",
            "Loss in iteration: 195 is: 2.802307367324829\n",
            "Loss in iteration: 196 is: 2.80230450630188\n",
            "Loss in iteration: 197 is: 2.8023014068603516\n",
            "Loss in iteration: 198 is: 2.8022992610931396\n",
            "Loss in iteration: 199 is: 2.8022961616516113\n",
            "Loss in iteration: 200 is: 2.802293300628662\n",
            "Loss in iteration: 201 is: 2.802290201187134\n",
            "Loss in iteration: 202 is: 2.8022878170013428\n",
            "Loss in iteration: 203 is: 2.8022849559783936\n",
            "Loss in iteration: 204 is: 2.8022823333740234\n",
            "Loss in iteration: 205 is: 2.802279233932495\n",
            "Loss in iteration: 206 is: 2.802277088165283\n",
            "Loss in iteration: 207 is: 2.802273988723755\n",
            "Loss in iteration: 208 is: 2.8022711277008057\n",
            "Loss in iteration: 209 is: 2.8022689819335938\n",
            "Loss in iteration: 210 is: 2.8022656440734863\n",
            "Loss in iteration: 211 is: 2.802263021469116\n",
            "Loss in iteration: 212 is: 2.802260398864746\n",
            "Loss in iteration: 213 is: 2.802257537841797\n",
            "Loss in iteration: 214 is: 2.8022544384002686\n",
            "Loss in iteration: 215 is: 2.8022513389587402\n",
            "Loss in iteration: 216 is: 2.8022496700286865\n",
            "Loss in iteration: 217 is: 2.80224609375\n",
            "Loss in iteration: 218 is: 2.80224347114563\n",
            "Loss in iteration: 219 is: 2.8022406101226807\n",
            "Loss in iteration: 220 is: 2.8022379875183105\n",
            "Loss in iteration: 221 is: 2.8022353649139404\n",
            "Loss in iteration: 222 is: 2.8022327423095703\n",
            "Loss in iteration: 223 is: 2.802229404449463\n",
            "Loss in iteration: 224 is: 2.8022265434265137\n",
            "Loss in iteration: 225 is: 2.8022241592407227\n",
            "Loss in iteration: 226 is: 2.8022212982177734\n",
            "Loss in iteration: 227 is: 2.802218437194824\n",
            "Loss in iteration: 228 is: 2.802216053009033\n",
            "Loss in iteration: 229 is: 2.802212953567505\n",
            "Loss in iteration: 230 is: 2.8022103309631348\n",
            "Loss in iteration: 231 is: 2.8022077083587646\n",
            "Loss in iteration: 232 is: 2.8022055625915527\n",
            "Loss in iteration: 233 is: 2.8022022247314453\n",
            "Loss in iteration: 234 is: 2.802199363708496\n",
            "Loss in iteration: 235 is: 2.802196979522705\n",
            "Loss in iteration: 236 is: 2.802194118499756\n",
            "Loss in iteration: 237 is: 2.8021914958953857\n",
            "Loss in iteration: 238 is: 2.8021883964538574\n",
            "Loss in iteration: 239 is: 2.802185535430908\n",
            "Loss in iteration: 240 is: 2.802182674407959\n",
            "Loss in iteration: 241 is: 2.802180051803589\n",
            "Loss in iteration: 242 is: 2.8021774291992188\n",
            "Loss in iteration: 243 is: 2.8021750450134277\n",
            "Loss in iteration: 244 is: 2.8021726608276367\n",
            "Loss in iteration: 245 is: 2.8021693229675293\n",
            "Loss in iteration: 246 is: 2.802166700363159\n",
            "Loss in iteration: 247 is: 2.80216383934021\n",
            "Loss in iteration: 248 is: 2.80216121673584\n",
            "Loss in iteration: 249 is: 2.8021581172943115\n",
            "Loss in iteration: 250 is: 2.8021554946899414\n",
            "Loss in iteration: 251 is: 2.8021528720855713\n",
            "Loss in iteration: 252 is: 2.802150249481201\n",
            "Loss in iteration: 253 is: 2.802147626876831\n",
            "Loss in iteration: 254 is: 2.802145004272461\n",
            "Loss in iteration: 255 is: 2.8021421432495117\n",
            "Loss in iteration: 256 is: 2.8021392822265625\n",
            "Loss in iteration: 257 is: 2.8021364212036133\n",
            "Loss in iteration: 258 is: 2.8021340370178223\n",
            "Loss in iteration: 259 is: 2.8021316528320312\n",
            "Loss in iteration: 260 is: 2.802128553390503\n",
            "Loss in iteration: 261 is: 2.8021256923675537\n",
            "Loss in iteration: 262 is: 2.802123546600342\n",
            "Loss in iteration: 263 is: 2.8021204471588135\n",
            "Loss in iteration: 264 is: 2.8021178245544434\n",
            "Loss in iteration: 265 is: 2.802114963531494\n",
            "Loss in iteration: 266 is: 2.802111864089966\n",
            "Loss in iteration: 267 is: 2.802110195159912\n",
            "Loss in iteration: 268 is: 2.8021066188812256\n",
            "Loss in iteration: 269 is: 2.8021039962768555\n",
            "Loss in iteration: 270 is: 2.8021013736724854\n",
            "Loss in iteration: 271 is: 2.8020987510681152\n",
            "Loss in iteration: 272 is: 2.802095651626587\n",
            "Loss in iteration: 273 is: 2.802093029022217\n",
            "Loss in iteration: 274 is: 2.802090644836426\n",
            "Loss in iteration: 275 is: 2.8020877838134766\n",
            "Loss in iteration: 276 is: 2.8020851612091064\n",
            "Loss in iteration: 277 is: 2.8020827770233154\n",
            "Loss in iteration: 278 is: 2.802079916000366\n",
            "Loss in iteration: 279 is: 2.802077054977417\n",
            "Loss in iteration: 280 is: 2.8020741939544678\n",
            "Loss in iteration: 281 is: 2.8020718097686768\n",
            "Loss in iteration: 282 is: 2.8020687103271484\n",
            "Loss in iteration: 283 is: 2.8020668029785156\n",
            "Loss in iteration: 284 is: 2.8020637035369873\n",
            "Loss in iteration: 285 is: 2.802061080932617\n",
            "Loss in iteration: 286 is: 2.802058458328247\n",
            "Loss in iteration: 287 is: 2.802055835723877\n",
            "Loss in iteration: 288 is: 2.8020529747009277\n",
            "Loss in iteration: 289 is: 2.8020503520965576\n",
            "Loss in iteration: 290 is: 2.8020482063293457\n",
            "Loss in iteration: 291 is: 2.8020448684692383\n",
            "Loss in iteration: 292 is: 2.802042245864868\n",
            "Loss in iteration: 293 is: 2.802039623260498\n",
            "Loss in iteration: 294 is: 2.802036762237549\n",
            "Loss in iteration: 295 is: 2.802034378051758\n",
            "Loss in iteration: 296 is: 2.8020312786102295\n",
            "Loss in iteration: 297 is: 2.8020286560058594\n",
            "Loss in iteration: 298 is: 2.8020260334014893\n",
            "Loss in iteration: 299 is: 2.802023410797119\n",
            "Loss in iteration: 300 is: 2.80202054977417\n",
            "Loss in iteration: 301 is: 2.802018404006958\n",
            "Loss in iteration: 302 is: 2.802015781402588\n",
            "Loss in iteration: 303 is: 2.8020129203796387\n",
            "Loss in iteration: 304 is: 2.8020100593566895\n",
            "Loss in iteration: 305 is: 2.8020076751708984\n",
            "Loss in iteration: 306 is: 2.8020050525665283\n",
            "Loss in iteration: 307 is: 2.802002191543579\n",
            "Loss in iteration: 308 is: 2.801999568939209\n",
            "Loss in iteration: 309 is: 2.801996946334839\n",
            "Loss in iteration: 310 is: 2.801994562149048\n",
            "Loss in iteration: 311 is: 2.8019917011260986\n",
            "Loss in iteration: 312 is: 2.8019888401031494\n",
            "Loss in iteration: 313 is: 2.8019862174987793\n",
            "Loss in iteration: 314 is: 2.8019838333129883\n",
            "Loss in iteration: 315 is: 2.801980972290039\n",
            "Loss in iteration: 316 is: 2.801978588104248\n",
            "Loss in iteration: 317 is: 2.8019754886627197\n",
            "Loss in iteration: 318 is: 2.8019728660583496\n",
            "Loss in iteration: 319 is: 2.8019704818725586\n",
            "Loss in iteration: 320 is: 2.801967144012451\n",
            "Loss in iteration: 321 is: 2.80196475982666\n",
            "Loss in iteration: 322 is: 2.801962375640869\n",
            "Loss in iteration: 323 is: 2.801959991455078\n",
            "Loss in iteration: 324 is: 2.801957130432129\n",
            "Loss in iteration: 325 is: 2.801954507827759\n",
            "Loss in iteration: 326 is: 2.8019516468048096\n",
            "Loss in iteration: 327 is: 2.8019492626190186\n",
            "Loss in iteration: 328 is: 2.8019464015960693\n",
            "Loss in iteration: 329 is: 2.8019440174102783\n",
            "Loss in iteration: 330 is: 2.8019418716430664\n",
            "Loss in iteration: 331 is: 2.801938772201538\n",
            "Loss in iteration: 332 is: 2.801936388015747\n",
            "Loss in iteration: 333 is: 2.801933526992798\n",
            "Loss in iteration: 334 is: 2.8019309043884277\n",
            "Loss in iteration: 335 is: 2.8019282817840576\n",
            "Loss in iteration: 336 is: 2.8019256591796875\n",
            "Loss in iteration: 337 is: 2.8019230365753174\n",
            "Loss in iteration: 338 is: 2.8019204139709473\n",
            "Loss in iteration: 339 is: 2.801917791366577\n",
            "Loss in iteration: 340 is: 2.801915168762207\n",
            "Loss in iteration: 341 is: 2.801912784576416\n",
            "Loss in iteration: 342 is: 2.8019096851348877\n",
            "Loss in iteration: 343 is: 2.801907539367676\n",
            "Loss in iteration: 344 is: 2.8019044399261475\n",
            "Loss in iteration: 345 is: 2.8019022941589355\n",
            "Loss in iteration: 346 is: 2.801898956298828\n",
            "Loss in iteration: 347 is: 2.801896333694458\n",
            "Loss in iteration: 348 is: 2.801893949508667\n",
            "Loss in iteration: 349 is: 2.801891326904297\n",
            "Loss in iteration: 350 is: 2.801888942718506\n",
            "Loss in iteration: 351 is: 2.801886558532715\n",
            "Loss in iteration: 352 is: 2.8018836975097656\n",
            "Loss in iteration: 353 is: 2.8018810749053955\n",
            "Loss in iteration: 354 is: 2.8018784523010254\n",
            "Loss in iteration: 355 is: 2.801875591278076\n",
            "Loss in iteration: 356 is: 2.8018734455108643\n",
            "Loss in iteration: 357 is: 2.801870584487915\n",
            "Loss in iteration: 358 is: 2.801867723464966\n",
            "Loss in iteration: 359 is: 2.801865339279175\n",
            "Loss in iteration: 360 is: 2.801862955093384\n",
            "Loss in iteration: 361 is: 2.8018603324890137\n",
            "Loss in iteration: 362 is: 2.8018577098846436\n",
            "Loss in iteration: 363 is: 2.8018550872802734\n",
            "Loss in iteration: 364 is: 2.801852226257324\n",
            "Loss in iteration: 365 is: 2.801849842071533\n",
            "Loss in iteration: 366 is: 2.801847457885742\n",
            "Loss in iteration: 367 is: 2.801844596862793\n",
            "Loss in iteration: 368 is: 2.801841974258423\n",
            "Loss in iteration: 369 is: 2.8018393516540527\n",
            "Loss in iteration: 370 is: 2.8018369674682617\n",
            "Loss in iteration: 371 is: 2.8018343448638916\n",
            "Loss in iteration: 372 is: 2.8018317222595215\n",
            "Loss in iteration: 373 is: 2.8018293380737305\n",
            "Loss in iteration: 374 is: 2.8018267154693604\n",
            "Loss in iteration: 375 is: 2.8018240928649902\n",
            "Loss in iteration: 376 is: 2.801821708679199\n",
            "Loss in iteration: 377 is: 2.80181884765625\n",
            "Loss in iteration: 378 is: 2.801816463470459\n",
            "Loss in iteration: 379 is: 2.8018136024475098\n",
            "Loss in iteration: 380 is: 2.8018109798431396\n",
            "Loss in iteration: 381 is: 2.8018083572387695\n",
            "Loss in iteration: 382 is: 2.8018059730529785\n",
            "Loss in iteration: 383 is: 2.8018033504486084\n",
            "Loss in iteration: 384 is: 2.8018009662628174\n",
            "Loss in iteration: 385 is: 2.801798105239868\n",
            "Loss in iteration: 386 is: 2.801795482635498\n",
            "Loss in iteration: 387 is: 2.801793098449707\n",
            "Loss in iteration: 388 is: 2.801790475845337\n",
            "Loss in iteration: 389 is: 2.8017876148223877\n",
            "Loss in iteration: 390 is: 2.801785469055176\n",
            "Loss in iteration: 391 is: 2.8017826080322266\n",
            "Loss in iteration: 392 is: 2.8017804622650146\n",
            "Loss in iteration: 393 is: 2.8017778396606445\n",
            "Loss in iteration: 394 is: 2.8017752170562744\n",
            "Loss in iteration: 395 is: 2.8017725944519043\n",
            "Loss in iteration: 396 is: 2.801769971847534\n",
            "Loss in iteration: 397 is: 2.801767349243164\n",
            "Loss in iteration: 398 is: 2.801764726638794\n",
            "Loss in iteration: 399 is: 2.801762104034424\n",
            "Loss in iteration: 400 is: 2.801759719848633\n",
            "Loss in iteration: 401 is: 2.801757574081421\n",
            "Loss in iteration: 402 is: 2.801754951477051\n",
            "Loss in iteration: 403 is: 2.8017520904541016\n",
            "Loss in iteration: 404 is: 2.8017497062683105\n",
            "Loss in iteration: 405 is: 2.8017468452453613\n",
            "Loss in iteration: 406 is: 2.8017444610595703\n",
            "Loss in iteration: 407 is: 2.801741600036621\n",
            "Loss in iteration: 408 is: 2.80173921585083\n",
            "Loss in iteration: 409 is: 2.80173659324646\n",
            "Loss in iteration: 410 is: 2.801734209060669\n",
            "Loss in iteration: 411 is: 2.8017313480377197\n",
            "Loss in iteration: 412 is: 2.8017289638519287\n",
            "Loss in iteration: 413 is: 2.8017265796661377\n",
            "Loss in iteration: 414 is: 2.8017241954803467\n",
            "Loss in iteration: 415 is: 2.8017215728759766\n",
            "Loss in iteration: 416 is: 2.8017191886901855\n",
            "Loss in iteration: 417 is: 2.8017163276672363\n",
            "Loss in iteration: 418 is: 2.801713705062866\n",
            "Loss in iteration: 419 is: 2.801711320877075\n",
            "Loss in iteration: 420 is: 2.801708936691284\n",
            "Loss in iteration: 421 is: 2.801706314086914\n",
            "Loss in iteration: 422 is: 2.801703453063965\n",
            "Loss in iteration: 423 is: 2.801701068878174\n",
            "Loss in iteration: 424 is: 2.801698684692383\n",
            "Loss in iteration: 425 is: 2.8016960620880127\n",
            "Loss in iteration: 426 is: 2.8016936779022217\n",
            "Loss in iteration: 427 is: 2.8016912937164307\n",
            "Loss in iteration: 428 is: 2.8016884326934814\n",
            "Loss in iteration: 429 is: 2.8016858100891113\n",
            "Loss in iteration: 430 is: 2.8016834259033203\n",
            "Loss in iteration: 431 is: 2.8016812801361084\n",
            "Loss in iteration: 432 is: 2.8016788959503174\n",
            "Loss in iteration: 433 is: 2.801675796508789\n",
            "Loss in iteration: 434 is: 2.80167293548584\n",
            "Loss in iteration: 435 is: 2.801670789718628\n",
            "Loss in iteration: 436 is: 2.801668167114258\n",
            "Loss in iteration: 437 is: 2.801666021347046\n",
            "Loss in iteration: 438 is: 2.8016631603240967\n",
            "Loss in iteration: 439 is: 2.8016607761383057\n",
            "Loss in iteration: 440 is: 2.8016581535339355\n",
            "Loss in iteration: 441 is: 2.8016557693481445\n",
            "Loss in iteration: 442 is: 2.801652669906616\n",
            "Loss in iteration: 443 is: 2.8016505241394043\n",
            "Loss in iteration: 444 is: 2.8016486167907715\n",
            "Loss in iteration: 445 is: 2.8016464710235596\n",
            "Loss in iteration: 446 is: 2.801642894744873\n",
            "Loss in iteration: 447 is: 2.8016409873962402\n",
            "Loss in iteration: 448 is: 2.801637887954712\n",
            "Loss in iteration: 449 is: 2.8016357421875\n",
            "Loss in iteration: 450 is: 2.801633358001709\n",
            "Loss in iteration: 451 is: 2.801630735397339\n",
            "Loss in iteration: 452 is: 2.8016278743743896\n",
            "Loss in iteration: 453 is: 2.8016252517700195\n",
            "Loss in iteration: 454 is: 2.8016233444213867\n",
            "Loss in iteration: 455 is: 2.8016204833984375\n",
            "Loss in iteration: 456 is: 2.8016180992126465\n",
            "Loss in iteration: 457 is: 2.8016157150268555\n",
            "Loss in iteration: 458 is: 2.8016130924224854\n",
            "Loss in iteration: 459 is: 2.8016109466552734\n",
            "Loss in iteration: 460 is: 2.801608085632324\n",
            "Loss in iteration: 461 is: 2.801605701446533\n",
            "Loss in iteration: 462 is: 2.801602840423584\n",
            "Loss in iteration: 463 is: 2.801600456237793\n",
            "Loss in iteration: 464 is: 2.801598072052002\n",
            "Loss in iteration: 465 is: 2.801595687866211\n",
            "Loss in iteration: 466 is: 2.80159330368042\n",
            "Loss in iteration: 467 is: 2.801591157913208\n",
            "Loss in iteration: 468 is: 2.801588296890259\n",
            "Loss in iteration: 469 is: 2.8015854358673096\n",
            "Loss in iteration: 470 is: 2.8015832901000977\n",
            "Loss in iteration: 471 is: 2.8015806674957275\n",
            "Loss in iteration: 472 is: 2.8015782833099365\n",
            "Loss in iteration: 473 is: 2.8015756607055664\n",
            "Loss in iteration: 474 is: 2.8015735149383545\n",
            "Loss in iteration: 475 is: 2.8015713691711426\n",
            "Loss in iteration: 476 is: 2.8015687465667725\n",
            "Loss in iteration: 477 is: 2.8015661239624023\n",
            "Loss in iteration: 478 is: 2.801563262939453\n",
            "Loss in iteration: 479 is: 2.801560640335083\n",
            "Loss in iteration: 480 is: 2.801558256149292\n",
            "Loss in iteration: 481 is: 2.801555871963501\n",
            "Loss in iteration: 482 is: 2.80155348777771\n",
            "Loss in iteration: 483 is: 2.801551103591919\n",
            "Loss in iteration: 484 is: 2.801548480987549\n",
            "Loss in iteration: 485 is: 2.8015458583831787\n",
            "Loss in iteration: 486 is: 2.801543712615967\n",
            "Loss in iteration: 487 is: 2.8015410900115967\n",
            "Loss in iteration: 488 is: 2.8015387058258057\n",
            "Loss in iteration: 489 is: 2.8015358448028564\n",
            "Loss in iteration: 490 is: 2.8015336990356445\n",
            "Loss in iteration: 491 is: 2.8015315532684326\n",
            "Loss in iteration: 492 is: 2.8015284538269043\n",
            "Loss in iteration: 493 is: 2.8015263080596924\n",
            "Loss in iteration: 494 is: 2.801523447036743\n",
            "Loss in iteration: 495 is: 2.8015215396881104\n",
            "Loss in iteration: 496 is: 2.801518678665161\n",
            "Loss in iteration: 497 is: 2.80151629447937\n",
            "Loss in iteration: 498 is: 2.801514148712158\n",
            "Loss in iteration: 499 is: 2.801511764526367\n",
            "Loss in iteration: 500 is: 2.801509141921997\n",
            "Loss in iteration: 501 is: 2.801506519317627\n",
            "Loss in iteration: 502 is: 2.801504135131836\n",
            "Loss in iteration: 503 is: 2.801501750946045\n",
            "Loss in iteration: 504 is: 2.801499366760254\n",
            "Loss in iteration: 505 is: 2.801496744155884\n",
            "Loss in iteration: 506 is: 2.8014941215515137\n",
            "Loss in iteration: 507 is: 2.8014919757843018\n",
            "Loss in iteration: 508 is: 2.8014893531799316\n",
            "Loss in iteration: 509 is: 2.8014872074127197\n",
            "Loss in iteration: 510 is: 2.8014848232269287\n",
            "Loss in iteration: 511 is: 2.8014822006225586\n",
            "Loss in iteration: 512 is: 2.8014800548553467\n",
            "Loss in iteration: 513 is: 2.8014774322509766\n",
            "Loss in iteration: 514 is: 2.8014748096466064\n",
            "Loss in iteration: 515 is: 2.8014726638793945\n",
            "Loss in iteration: 516 is: 2.8014702796936035\n",
            "Loss in iteration: 517 is: 2.801467180252075\n",
            "Loss in iteration: 518 is: 2.8014652729034424\n",
            "Loss in iteration: 519 is: 2.801462411880493\n",
            "Loss in iteration: 520 is: 2.8014602661132812\n",
            "Loss in iteration: 521 is: 2.8014581203460693\n",
            "Loss in iteration: 522 is: 2.801455497741699\n",
            "Loss in iteration: 523 is: 2.801453113555908\n",
            "Loss in iteration: 524 is: 2.801450490951538\n",
            "Loss in iteration: 525 is: 2.801448106765747\n",
            "Loss in iteration: 526 is: 2.801445722579956\n",
            "Loss in iteration: 527 is: 2.801443338394165\n",
            "Loss in iteration: 528 is: 2.8014402389526367\n",
            "Loss in iteration: 529 is: 2.801438570022583\n",
            "Loss in iteration: 530 is: 2.801436185836792\n",
            "Loss in iteration: 531 is: 2.801433801651001\n",
            "Loss in iteration: 532 is: 2.8014307022094727\n",
            "Loss in iteration: 533 is: 2.8014285564422607\n",
            "Loss in iteration: 534 is: 2.8014256954193115\n",
            "Loss in iteration: 535 is: 2.801424026489258\n",
            "Loss in iteration: 536 is: 2.801421642303467\n",
            "Loss in iteration: 537 is: 2.8014187812805176\n",
            "Loss in iteration: 538 is: 2.8014166355133057\n",
            "Loss in iteration: 539 is: 2.8014140129089355\n",
            "Loss in iteration: 540 is: 2.8014118671417236\n",
            "Loss in iteration: 541 is: 2.8014087677001953\n",
            "Loss in iteration: 542 is: 2.8014066219329834\n",
            "Loss in iteration: 543 is: 2.8014042377471924\n",
            "Loss in iteration: 544 is: 2.8014018535614014\n",
            "Loss in iteration: 545 is: 2.8013997077941895\n",
            "Loss in iteration: 546 is: 2.8013973236083984\n",
            "Loss in iteration: 547 is: 2.8013949394226074\n",
            "Loss in iteration: 548 is: 2.801392078399658\n",
            "Loss in iteration: 549 is: 2.8013901710510254\n",
            "Loss in iteration: 550 is: 2.8013877868652344\n",
            "Loss in iteration: 551 is: 2.8013851642608643\n",
            "Loss in iteration: 552 is: 2.801382541656494\n",
            "Loss in iteration: 553 is: 2.8013806343078613\n",
            "Loss in iteration: 554 is: 2.801377773284912\n",
            "Loss in iteration: 555 is: 2.801375389099121\n",
            "Loss in iteration: 556 is: 2.801373243331909\n",
            "Loss in iteration: 557 is: 2.801370859146118\n",
            "Loss in iteration: 558 is: 2.801368474960327\n",
            "Loss in iteration: 559 is: 2.801365613937378\n",
            "Loss in iteration: 560 is: 2.801363468170166\n",
            "Loss in iteration: 561 is: 2.801361083984375\n",
            "Loss in iteration: 562 is: 2.801358699798584\n",
            "Loss in iteration: 563 is: 2.801356315612793\n",
            "Loss in iteration: 564 is: 2.80135440826416\n",
            "Loss in iteration: 565 is: 2.801351308822632\n",
            "Loss in iteration: 566 is: 2.801349401473999\n",
            "Loss in iteration: 567 is: 2.801347017288208\n",
            "Loss in iteration: 568 is: 2.801344156265259\n",
            "Loss in iteration: 569 is: 2.801342010498047\n",
            "Loss in iteration: 570 is: 2.801339864730835\n",
            "Loss in iteration: 571 is: 2.801337480545044\n",
            "Loss in iteration: 572 is: 2.801335334777832\n",
            "Loss in iteration: 573 is: 2.8013322353363037\n",
            "Loss in iteration: 574 is: 2.8013298511505127\n",
            "Loss in iteration: 575 is: 2.8013274669647217\n",
            "Loss in iteration: 576 is: 2.801325798034668\n",
            "Loss in iteration: 577 is: 2.8013229370117188\n",
            "Loss in iteration: 578 is: 2.801321029663086\n",
            "Loss in iteration: 579 is: 2.801318407058716\n",
            "Loss in iteration: 580 is: 2.8013157844543457\n",
            "Loss in iteration: 581 is: 2.8013134002685547\n",
            "Loss in iteration: 582 is: 2.8013110160827637\n",
            "Loss in iteration: 583 is: 2.8013086318969727\n",
            "Loss in iteration: 584 is: 2.8013062477111816\n",
            "Loss in iteration: 585 is: 2.801304340362549\n",
            "Loss in iteration: 586 is: 2.8013014793395996\n",
            "Loss in iteration: 587 is: 2.8012993335723877\n",
            "Loss in iteration: 588 is: 2.8012969493865967\n",
            "Loss in iteration: 589 is: 2.8012943267822266\n",
            "Loss in iteration: 590 is: 2.8012914657592773\n",
            "Loss in iteration: 591 is: 2.8012893199920654\n",
            "Loss in iteration: 592 is: 2.8012874126434326\n",
            "Loss in iteration: 593 is: 2.8012850284576416\n",
            "Loss in iteration: 594 is: 2.8012828826904297\n",
            "Loss in iteration: 595 is: 2.8012804985046387\n",
            "Loss in iteration: 596 is: 2.8012778759002686\n",
            "Loss in iteration: 597 is: 2.8012757301330566\n",
            "Loss in iteration: 598 is: 2.8012731075286865\n",
            "Loss in iteration: 599 is: 2.8012707233428955\n",
            "Loss in iteration: 600 is: 2.801269054412842\n",
            "Loss in iteration: 601 is: 2.8012661933898926\n",
            "Loss in iteration: 602 is: 2.8012640476226807\n",
            "Loss in iteration: 603 is: 2.8012614250183105\n",
            "Loss in iteration: 604 is: 2.8012592792510986\n",
            "Loss in iteration: 605 is: 2.8012568950653076\n",
            "Loss in iteration: 606 is: 2.8012540340423584\n",
            "Loss in iteration: 607 is: 2.8012518882751465\n",
            "Loss in iteration: 608 is: 2.8012497425079346\n",
            "Loss in iteration: 609 is: 2.8012475967407227\n",
            "Loss in iteration: 610 is: 2.8012449741363525\n",
            "Loss in iteration: 611 is: 2.8012423515319824\n",
            "Loss in iteration: 612 is: 2.8012404441833496\n",
            "Loss in iteration: 613 is: 2.8012380599975586\n",
            "Loss in iteration: 614 is: 2.8012356758117676\n",
            "Loss in iteration: 615 is: 2.8012332916259766\n",
            "Loss in iteration: 616 is: 2.8012309074401855\n",
            "Loss in iteration: 617 is: 2.8012280464172363\n",
            "Loss in iteration: 618 is: 2.8012266159057617\n",
            "Loss in iteration: 619 is: 2.8012237548828125\n",
            "Loss in iteration: 620 is: 2.8012216091156006\n",
            "Loss in iteration: 621 is: 2.8012192249298096\n",
            "Loss in iteration: 622 is: 2.8012170791625977\n",
            "Loss in iteration: 623 is: 2.8012144565582275\n",
            "Loss in iteration: 624 is: 2.8012123107910156\n",
            "Loss in iteration: 625 is: 2.8012099266052246\n",
            "Loss in iteration: 626 is: 2.8012073040008545\n",
            "Loss in iteration: 627 is: 2.8012051582336426\n",
            "Loss in iteration: 628 is: 2.8012027740478516\n",
            "Loss in iteration: 629 is: 2.8012003898620605\n",
            "Loss in iteration: 630 is: 2.8011982440948486\n",
            "Loss in iteration: 631 is: 2.8011958599090576\n",
            "Loss in iteration: 632 is: 2.8011937141418457\n",
            "Loss in iteration: 633 is: 2.8011910915374756\n",
            "Loss in iteration: 634 is: 2.8011887073516846\n",
            "Loss in iteration: 635 is: 2.8011863231658936\n",
            "Loss in iteration: 636 is: 2.8011841773986816\n",
            "Loss in iteration: 637 is: 2.8011817932128906\n",
            "Loss in iteration: 638 is: 2.8011796474456787\n",
            "Loss in iteration: 639 is: 2.8011772632598877\n",
            "Loss in iteration: 640 is: 2.8011748790740967\n",
            "Loss in iteration: 641 is: 2.801172971725464\n",
            "Loss in iteration: 642 is: 2.8011703491210938\n",
            "Loss in iteration: 643 is: 2.801168203353882\n",
            "Loss in iteration: 644 is: 2.8011655807495117\n",
            "Loss in iteration: 645 is: 2.8011631965637207\n",
            "Loss in iteration: 646 is: 2.801161289215088\n",
            "Loss in iteration: 647 is: 2.801158905029297\n",
            "Loss in iteration: 648 is: 2.801156520843506\n",
            "Loss in iteration: 649 is: 2.801154136657715\n",
            "Loss in iteration: 650 is: 2.801152467727661\n",
            "Loss in iteration: 651 is: 2.801149368286133\n",
            "Loss in iteration: 652 is: 2.801147222518921\n",
            "Loss in iteration: 653 is: 2.801145076751709\n",
            "Loss in iteration: 654 is: 2.801142692565918\n",
            "Loss in iteration: 655 is: 2.801140069961548\n",
            "Loss in iteration: 656 is: 2.801137685775757\n",
            "Loss in iteration: 657 is: 2.801135540008545\n",
            "Loss in iteration: 658 is: 2.801133632659912\n",
            "Loss in iteration: 659 is: 2.801130771636963\n",
            "Loss in iteration: 660 is: 2.801129102706909\n",
            "Loss in iteration: 661 is: 2.801126480102539\n",
            "Loss in iteration: 662 is: 2.801124095916748\n",
            "Loss in iteration: 663 is: 2.801121711730957\n",
            "Loss in iteration: 664 is: 2.801119327545166\n",
            "Loss in iteration: 665 is: 2.801116943359375\n",
            "Loss in iteration: 666 is: 2.801114797592163\n",
            "Loss in iteration: 667 is: 2.801112413406372\n",
            "Loss in iteration: 668 is: 2.8011105060577393\n",
            "Loss in iteration: 669 is: 2.8011081218719482\n",
            "Loss in iteration: 670 is: 2.801105499267578\n",
            "Loss in iteration: 671 is: 2.8011035919189453\n",
            "Loss in iteration: 672 is: 2.8011012077331543\n",
            "Loss in iteration: 673 is: 2.801098585128784\n",
            "Loss in iteration: 674 is: 2.8010966777801514\n",
            "Loss in iteration: 675 is: 2.8010942935943604\n",
            "Loss in iteration: 676 is: 2.8010919094085693\n",
            "Loss in iteration: 677 is: 2.8010897636413574\n",
            "Loss in iteration: 678 is: 2.8010873794555664\n",
            "Loss in iteration: 679 is: 2.8010849952697754\n",
            "Loss in iteration: 680 is: 2.8010828495025635\n",
            "Loss in iteration: 681 is: 2.8010807037353516\n",
            "Loss in iteration: 682 is: 2.8010780811309814\n",
            "Loss in iteration: 683 is: 2.8010761737823486\n",
            "Loss in iteration: 684 is: 2.8010740280151367\n",
            "Loss in iteration: 685 is: 2.8010711669921875\n",
            "Loss in iteration: 686 is: 2.8010687828063965\n",
            "Loss in iteration: 687 is: 2.8010663986206055\n",
            "Loss in iteration: 688 is: 2.8010644912719727\n",
            "Loss in iteration: 689 is: 2.8010621070861816\n",
            "Loss in iteration: 690 is: 2.8010597229003906\n",
            "Loss in iteration: 691 is: 2.8010571002960205\n",
            "Loss in iteration: 692 is: 2.8010551929473877\n",
            "Loss in iteration: 693 is: 2.801053285598755\n",
            "Loss in iteration: 694 is: 2.801050901412964\n",
            "Loss in iteration: 695 is: 2.801048517227173\n",
            "Loss in iteration: 696 is: 2.801046371459961\n",
            "Loss in iteration: 697 is: 2.80104398727417\n",
            "Loss in iteration: 698 is: 2.801041603088379\n",
            "Loss in iteration: 699 is: 2.801039218902588\n",
            "Loss in iteration: 700 is: 2.801037073135376\n",
            "Loss in iteration: 701 is: 2.801035165786743\n",
            "Loss in iteration: 702 is: 2.801032781600952\n",
            "Loss in iteration: 703 is: 2.801030397415161\n",
            "Loss in iteration: 704 is: 2.80102801322937\n",
            "Loss in iteration: 705 is: 2.801025390625\n",
            "Loss in iteration: 706 is: 2.8010239601135254\n",
            "Loss in iteration: 707 is: 2.8010213375091553\n",
            "Loss in iteration: 708 is: 2.8010191917419434\n",
            "Loss in iteration: 709 is: 2.8010165691375732\n",
            "Loss in iteration: 710 is: 2.8010146617889404\n",
            "Loss in iteration: 711 is: 2.8010122776031494\n",
            "Loss in iteration: 712 is: 2.8010103702545166\n",
            "Loss in iteration: 713 is: 2.8010077476501465\n",
            "Loss in iteration: 714 is: 2.8010053634643555\n",
            "Loss in iteration: 715 is: 2.8010032176971436\n",
            "Loss in iteration: 716 is: 2.8010010719299316\n",
            "Loss in iteration: 717 is: 2.8009989261627197\n",
            "Loss in iteration: 718 is: 2.800996780395508\n",
            "Loss in iteration: 719 is: 2.8009941577911377\n",
            "Loss in iteration: 720 is: 2.800992012023926\n",
            "Loss in iteration: 721 is: 2.8009896278381348\n",
            "Loss in iteration: 722 is: 2.800987720489502\n",
            "Loss in iteration: 723 is: 2.800985813140869\n",
            "Loss in iteration: 724 is: 2.800983428955078\n",
            "Loss in iteration: 725 is: 2.800981044769287\n",
            "Loss in iteration: 726 is: 2.800978183746338\n",
            "Loss in iteration: 727 is: 2.800976037979126\n",
            "Loss in iteration: 728 is: 2.800974130630493\n",
            "Loss in iteration: 729 is: 2.8009719848632812\n",
            "Loss in iteration: 730 is: 2.8009696006774902\n",
            "Loss in iteration: 731 is: 2.8009676933288574\n",
            "Loss in iteration: 732 is: 2.8009650707244873\n",
            "Loss in iteration: 733 is: 2.8009629249572754\n",
            "Loss in iteration: 734 is: 2.8009607791900635\n",
            "Loss in iteration: 735 is: 2.8009586334228516\n",
            "Loss in iteration: 736 is: 2.8009560108184814\n",
            "Loss in iteration: 737 is: 2.8009538650512695\n",
            "Loss in iteration: 738 is: 2.8009517192840576\n",
            "Loss in iteration: 739 is: 2.800950050354004\n",
            "Loss in iteration: 740 is: 2.800947427749634\n",
            "Loss in iteration: 741 is: 2.8009448051452637\n",
            "Loss in iteration: 742 is: 2.80094313621521\n",
            "Loss in iteration: 743 is: 2.80094051361084\n",
            "Loss in iteration: 744 is: 2.800938129425049\n",
            "Loss in iteration: 745 is: 2.800936222076416\n",
            "Loss in iteration: 746 is: 2.800933837890625\n",
            "Loss in iteration: 747 is: 2.800931453704834\n",
            "Loss in iteration: 748 is: 2.800929307937622\n",
            "Loss in iteration: 749 is: 2.8009274005889893\n",
            "Loss in iteration: 750 is: 2.800924777984619\n",
            "Loss in iteration: 751 is: 2.8009228706359863\n",
            "Loss in iteration: 752 is: 2.8009204864501953\n",
            "Loss in iteration: 753 is: 2.8009185791015625\n",
            "Loss in iteration: 754 is: 2.8009164333343506\n",
            "Loss in iteration: 755 is: 2.8009135723114014\n",
            "Loss in iteration: 756 is: 2.8009116649627686\n",
            "Loss in iteration: 757 is: 2.8009090423583984\n",
            "Loss in iteration: 758 is: 2.8009068965911865\n",
            "Loss in iteration: 759 is: 2.8009049892425537\n",
            "Loss in iteration: 760 is: 2.800902843475342\n",
            "Loss in iteration: 761 is: 2.800900459289551\n",
            "Loss in iteration: 762 is: 2.800898313522339\n",
            "Loss in iteration: 763 is: 2.800895929336548\n",
            "Loss in iteration: 764 is: 2.800894260406494\n",
            "Loss in iteration: 765 is: 2.800891399383545\n",
            "Loss in iteration: 766 is: 2.800889492034912\n",
            "Loss in iteration: 767 is: 2.800887107849121\n",
            "Loss in iteration: 768 is: 2.8008852005004883\n",
            "Loss in iteration: 769 is: 2.8008830547332764\n",
            "Loss in iteration: 770 is: 2.8008809089660645\n",
            "Loss in iteration: 771 is: 2.8008785247802734\n",
            "Loss in iteration: 772 is: 2.8008761405944824\n",
            "Loss in iteration: 773 is: 2.8008742332458496\n",
            "Loss in iteration: 774 is: 2.8008718490600586\n",
            "Loss in iteration: 775 is: 2.800869941711426\n",
            "Loss in iteration: 776 is: 2.8008673191070557\n",
            "Loss in iteration: 777 is: 2.800865411758423\n",
            "Loss in iteration: 778 is: 2.8008627891540527\n",
            "Loss in iteration: 779 is: 2.800860643386841\n",
            "Loss in iteration: 780 is: 2.800858497619629\n",
            "Loss in iteration: 781 is: 2.800856351852417\n",
            "Loss in iteration: 782 is: 2.800854206085205\n",
            "Loss in iteration: 783 is: 2.800852060317993\n",
            "Loss in iteration: 784 is: 2.8008499145507812\n",
            "Loss in iteration: 785 is: 2.8008477687835693\n",
            "Loss in iteration: 786 is: 2.8008456230163574\n",
            "Loss in iteration: 787 is: 2.8008432388305664\n",
            "Loss in iteration: 788 is: 2.8008413314819336\n",
            "Loss in iteration: 789 is: 2.8008382320404053\n",
            "Loss in iteration: 790 is: 2.8008368015289307\n",
            "Loss in iteration: 791 is: 2.8008344173431396\n",
            "Loss in iteration: 792 is: 2.800832748413086\n",
            "Loss in iteration: 793 is: 2.800830125808716\n",
            "Loss in iteration: 794 is: 2.800827741622925\n",
            "Loss in iteration: 795 is: 2.800825357437134\n",
            "Loss in iteration: 796 is: 2.800823450088501\n",
            "Loss in iteration: 797 is: 2.800821542739868\n",
            "Loss in iteration: 798 is: 2.800818920135498\n",
            "Loss in iteration: 799 is: 2.8008172512054443\n",
            "Loss in iteration: 800 is: 2.8008148670196533\n",
            "Loss in iteration: 801 is: 2.8008127212524414\n",
            "Loss in iteration: 802 is: 2.8008103370666504\n",
            "Loss in iteration: 803 is: 2.8008084297180176\n",
            "Loss in iteration: 804 is: 2.8008058071136475\n",
            "Loss in iteration: 805 is: 2.8008036613464355\n",
            "Loss in iteration: 806 is: 2.8008015155792236\n",
            "Loss in iteration: 807 is: 2.80079984664917\n",
            "Loss in iteration: 808 is: 2.800797462463379\n",
            "Loss in iteration: 809 is: 2.8007946014404297\n",
            "Loss in iteration: 810 is: 2.800793170928955\n",
            "Loss in iteration: 811 is: 2.8007912635803223\n",
            "Loss in iteration: 812 is: 2.800788402557373\n",
            "Loss in iteration: 813 is: 2.8007867336273193\n",
            "Loss in iteration: 814 is: 2.80078387260437\n",
            "Loss in iteration: 815 is: 2.8007826805114746\n",
            "Loss in iteration: 816 is: 2.8007800579071045\n",
            "Loss in iteration: 817 is: 2.8007779121398926\n",
            "Loss in iteration: 818 is: 2.8007755279541016\n",
            "Loss in iteration: 819 is: 2.8007733821868896\n",
            "Loss in iteration: 820 is: 2.800771474838257\n",
            "Loss in iteration: 821 is: 2.800769090652466\n",
            "Loss in iteration: 822 is: 2.800766944885254\n",
            "Loss in iteration: 823 is: 2.800764799118042\n",
            "Loss in iteration: 824 is: 2.8007631301879883\n",
            "Loss in iteration: 825 is: 2.800759792327881\n",
            "Loss in iteration: 826 is: 2.8007586002349854\n",
            "Loss in iteration: 827 is: 2.8007566928863525\n",
            "Loss in iteration: 828 is: 2.8007543087005615\n",
            "Loss in iteration: 829 is: 2.8007519245147705\n",
            "Loss in iteration: 830 is: 2.8007500171661377\n",
            "Loss in iteration: 831 is: 2.8007476329803467\n",
            "Loss in iteration: 832 is: 2.8007454872131348\n",
            "Loss in iteration: 833 is: 2.800743341445923\n",
            "Loss in iteration: 834 is: 2.8007407188415527\n",
            "Loss in iteration: 835 is: 2.80073881149292\n",
            "Loss in iteration: 836 is: 2.800736904144287\n",
            "Loss in iteration: 837 is: 2.800734758377075\n",
            "Loss in iteration: 838 is: 2.8007326126098633\n",
            "Loss in iteration: 839 is: 2.8007304668426514\n",
            "Loss in iteration: 840 is: 2.8007283210754395\n",
            "Loss in iteration: 841 is: 2.8007256984710693\n",
            "Loss in iteration: 842 is: 2.8007235527038574\n",
            "Loss in iteration: 843 is: 2.8007218837738037\n",
            "Loss in iteration: 844 is: 2.8007192611694336\n",
            "Loss in iteration: 845 is: 2.800717830657959\n",
            "Loss in iteration: 846 is: 2.800715684890747\n",
            "Loss in iteration: 847 is: 2.8007137775421143\n",
            "Loss in iteration: 848 is: 2.8007113933563232\n",
            "Loss in iteration: 849 is: 2.8007094860076904\n",
            "Loss in iteration: 850 is: 2.8007068634033203\n",
            "Loss in iteration: 851 is: 2.8007049560546875\n",
            "Loss in iteration: 852 is: 2.8007025718688965\n",
            "Loss in iteration: 853 is: 2.8007006645202637\n",
            "Loss in iteration: 854 is: 2.8006985187530518\n",
            "Loss in iteration: 855 is: 2.80069637298584\n",
            "Loss in iteration: 856 is: 2.800693988800049\n",
            "Loss in iteration: 857 is: 2.800692081451416\n",
            "Loss in iteration: 858 is: 2.800689697265625\n",
            "Loss in iteration: 859 is: 2.800687551498413\n",
            "Loss in iteration: 860 is: 2.800685405731201\n",
            "Loss in iteration: 861 is: 2.80068302154541\n",
            "Loss in iteration: 862 is: 2.8006811141967773\n",
            "Loss in iteration: 863 is: 2.8006789684295654\n",
            "Loss in iteration: 864 is: 2.8006768226623535\n",
            "Loss in iteration: 865 is: 2.8006749153137207\n",
            "Loss in iteration: 866 is: 2.8006725311279297\n",
            "Loss in iteration: 867 is: 2.800670862197876\n",
            "Loss in iteration: 868 is: 2.800668954849243\n",
            "Loss in iteration: 869 is: 2.800666093826294\n",
            "Loss in iteration: 870 is: 2.8006644248962402\n",
            "Loss in iteration: 871 is: 2.80066180229187\n",
            "Loss in iteration: 872 is: 2.800659418106079\n",
            "Loss in iteration: 873 is: 2.8006575107574463\n",
            "Loss in iteration: 874 is: 2.8006558418273926\n",
            "Loss in iteration: 875 is: 2.8006536960601807\n",
            "Loss in iteration: 876 is: 2.8006515502929688\n",
            "Loss in iteration: 877 is: 2.8006491661071777\n",
            "Loss in iteration: 878 is: 2.800647020339966\n",
            "Loss in iteration: 879 is: 2.800644874572754\n",
            "Loss in iteration: 880 is: 2.8006432056427\n",
            "Loss in iteration: 881 is: 2.8006410598754883\n",
            "Loss in iteration: 882 is: 2.8006386756896973\n",
            "Loss in iteration: 883 is: 2.8006362915039062\n",
            "Loss in iteration: 884 is: 2.8006343841552734\n",
            "Loss in iteration: 885 is: 2.8006327152252197\n",
            "Loss in iteration: 886 is: 2.8006300926208496\n",
            "Loss in iteration: 887 is: 2.8006279468536377\n",
            "Loss in iteration: 888 is: 2.800626277923584\n",
            "Loss in iteration: 889 is: 2.800624370574951\n",
            "Loss in iteration: 890 is: 2.80062198638916\n",
            "Loss in iteration: 891 is: 2.8006198406219482\n",
            "Loss in iteration: 892 is: 2.80061674118042\n",
            "Loss in iteration: 893 is: 2.8006155490875244\n",
            "Loss in iteration: 894 is: 2.8006134033203125\n",
            "Loss in iteration: 895 is: 2.8006112575531006\n",
            "Loss in iteration: 896 is: 2.8006093502044678\n",
            "Loss in iteration: 897 is: 2.800607204437256\n",
            "Loss in iteration: 898 is: 2.800605297088623\n",
            "Loss in iteration: 899 is: 2.800602674484253\n",
            "Loss in iteration: 900 is: 2.800600528717041\n",
            "Loss in iteration: 901 is: 2.800598621368408\n",
            "Loss in iteration: 902 is: 2.8005964756011963\n",
            "Loss in iteration: 903 is: 2.8005948066711426\n",
            "Loss in iteration: 904 is: 2.8005924224853516\n",
            "Loss in iteration: 905 is: 2.8005902767181396\n",
            "Loss in iteration: 906 is: 2.8005881309509277\n",
            "Loss in iteration: 907 is: 2.800585985183716\n",
            "Loss in iteration: 908 is: 2.800584077835083\n",
            "Loss in iteration: 909 is: 2.800581932067871\n",
            "Loss in iteration: 910 is: 2.800579786300659\n",
            "Loss in iteration: 911 is: 2.8005776405334473\n",
            "Loss in iteration: 912 is: 2.8005754947662354\n",
            "Loss in iteration: 913 is: 2.8005733489990234\n",
            "Loss in iteration: 914 is: 2.8005714416503906\n",
            "Loss in iteration: 915 is: 2.800569772720337\n",
            "Loss in iteration: 916 is: 2.8005669116973877\n",
            "Loss in iteration: 917 is: 2.800564765930176\n",
            "Loss in iteration: 918 is: 2.800563335418701\n",
            "Loss in iteration: 919 is: 2.8005611896514893\n",
            "Loss in iteration: 920 is: 2.8005590438842773\n",
            "Loss in iteration: 921 is: 2.8005568981170654\n",
            "Loss in iteration: 922 is: 2.8005545139312744\n",
            "Loss in iteration: 923 is: 2.8005530834198\n",
            "Loss in iteration: 924 is: 2.800550699234009\n",
            "Loss in iteration: 925 is: 2.800548791885376\n",
            "Loss in iteration: 926 is: 2.800546169281006\n",
            "Loss in iteration: 927 is: 2.800544261932373\n",
            "Loss in iteration: 928 is: 2.800542116165161\n",
            "Loss in iteration: 929 is: 2.800539970397949\n",
            "Loss in iteration: 930 is: 2.8005385398864746\n",
            "Loss in iteration: 931 is: 2.8005359172821045\n",
            "Loss in iteration: 932 is: 2.800534248352051\n",
            "Loss in iteration: 933 is: 2.8005318641662598\n",
            "Loss in iteration: 934 is: 2.800529956817627\n",
            "Loss in iteration: 935 is: 2.800527811050415\n",
            "Loss in iteration: 936 is: 2.8005259037017822\n",
            "Loss in iteration: 937 is: 2.800523519515991\n",
            "Loss in iteration: 938 is: 2.8005216121673584\n",
            "Loss in iteration: 939 is: 2.8005194664001465\n",
            "Loss in iteration: 940 is: 2.8005170822143555\n",
            "Loss in iteration: 941 is: 2.8005151748657227\n",
            "Loss in iteration: 942 is: 2.80051326751709\n",
            "Loss in iteration: 943 is: 2.800510883331299\n",
            "Loss in iteration: 944 is: 2.800509214401245\n",
            "Loss in iteration: 945 is: 2.800506830215454\n",
            "Loss in iteration: 946 is: 2.8005049228668213\n",
            "Loss in iteration: 947 is: 2.8005027770996094\n",
            "Loss in iteration: 948 is: 2.8005011081695557\n",
            "Loss in iteration: 949 is: 2.8004989624023438\n",
            "Loss in iteration: 950 is: 2.8004963397979736\n",
            "Loss in iteration: 951 is: 2.80049467086792\n",
            "Loss in iteration: 952 is: 2.800492525100708\n",
            "Loss in iteration: 953 is: 2.8004908561706543\n",
            "Loss in iteration: 954 is: 2.8004884719848633\n",
            "Loss in iteration: 955 is: 2.8004860877990723\n",
            "Loss in iteration: 956 is: 2.8004841804504395\n",
            "Loss in iteration: 957 is: 2.8004820346832275\n",
            "Loss in iteration: 958 is: 2.800480604171753\n",
            "Loss in iteration: 959 is: 2.8004777431488037\n",
            "Loss in iteration: 960 is: 2.800476312637329\n",
            "Loss in iteration: 961 is: 2.800474166870117\n",
            "Loss in iteration: 962 is: 2.8004720211029053\n",
            "Loss in iteration: 963 is: 2.8004698753356934\n",
            "Loss in iteration: 964 is: 2.8004684448242188\n",
            "Loss in iteration: 965 is: 2.8004658222198486\n",
            "Loss in iteration: 966 is: 2.8004636764526367\n",
            "Loss in iteration: 967 is: 2.800461530685425\n",
            "Loss in iteration: 968 is: 2.800459861755371\n",
            "Loss in iteration: 969 is: 2.80045747756958\n",
            "Loss in iteration: 970 is: 2.8004560470581055\n",
            "Loss in iteration: 971 is: 2.8004541397094727\n",
            "Loss in iteration: 972 is: 2.8004515171051025\n",
            "Loss in iteration: 973 is: 2.8004493713378906\n",
            "Loss in iteration: 974 is: 2.800447702407837\n",
            "Loss in iteration: 975 is: 2.800445556640625\n",
            "Loss in iteration: 976 is: 2.800443172454834\n",
            "Loss in iteration: 977 is: 2.800441265106201\n",
            "Loss in iteration: 978 is: 2.8004391193389893\n",
            "Loss in iteration: 979 is: 2.8004374504089355\n",
            "Loss in iteration: 980 is: 2.8004355430603027\n",
            "Loss in iteration: 981 is: 2.8004331588745117\n",
            "Loss in iteration: 982 is: 2.8004310131073\n",
            "Loss in iteration: 983 is: 2.800428867340088\n",
            "Loss in iteration: 984 is: 2.800426721572876\n",
            "Loss in iteration: 985 is: 2.8004250526428223\n",
            "Loss in iteration: 986 is: 2.8004226684570312\n",
            "Loss in iteration: 987 is: 2.8004209995269775\n",
            "Loss in iteration: 988 is: 2.8004188537597656\n",
            "Loss in iteration: 989 is: 2.800416946411133\n",
            "Loss in iteration: 990 is: 2.800414800643921\n",
            "Loss in iteration: 991 is: 2.800412654876709\n",
            "Loss in iteration: 992 is: 2.800410747528076\n",
            "Loss in iteration: 993 is: 2.800408363342285\n",
            "Loss in iteration: 994 is: 2.8004066944122314\n",
            "Loss in iteration: 995 is: 2.8004047870635986\n",
            "Loss in iteration: 996 is: 2.8004026412963867\n",
            "Loss in iteration: 997 is: 2.8004002571105957\n",
            "Loss in iteration: 998 is: 2.800398349761963\n",
            "Loss in iteration: 999 is: 2.80039644241333\n",
            "Loss in iteration: 1000 is: 2.800394058227539\n",
            "Loss in iteration: 1001 is: 2.8003923892974854\n",
            "Loss in iteration: 1002 is: 2.8003904819488525\n",
            "Loss in iteration: 1003 is: 2.8003880977630615\n",
            "Loss in iteration: 1004 is: 2.8003861904144287\n",
            "Loss in iteration: 1005 is: 2.800384283065796\n",
            "Loss in iteration: 1006 is: 2.800381898880005\n",
            "Loss in iteration: 1007 is: 2.8003807067871094\n",
            "Loss in iteration: 1008 is: 2.8003787994384766\n",
            "Loss in iteration: 1009 is: 2.8003764152526855\n",
            "Loss in iteration: 1010 is: 2.8003745079040527\n",
            "Loss in iteration: 1011 is: 2.8003721237182617\n",
            "Loss in iteration: 1012 is: 2.800370931625366\n",
            "Loss in iteration: 1013 is: 2.800368547439575\n",
            "Loss in iteration: 1014 is: 2.8003664016723633\n",
            "Loss in iteration: 1015 is: 2.800363779067993\n",
            "Loss in iteration: 1016 is: 2.8003623485565186\n",
            "Loss in iteration: 1017 is: 2.8003604412078857\n",
            "Loss in iteration: 1018 is: 2.800358533859253\n",
            "Loss in iteration: 1019 is: 2.800356388092041\n",
            "Loss in iteration: 1020 is: 2.80035400390625\n",
            "Loss in iteration: 1021 is: 2.800351858139038\n",
            "Loss in iteration: 1022 is: 2.800349712371826\n",
            "Loss in iteration: 1023 is: 2.8003478050231934\n",
            "Loss in iteration: 1024 is: 2.8003456592559814\n",
            "Loss in iteration: 1025 is: 2.800344228744507\n",
            "Loss in iteration: 1026 is: 2.800342082977295\n",
            "Loss in iteration: 1027 is: 2.800339937210083\n",
            "Loss in iteration: 1028 is: 2.80033802986145\n",
            "Loss in iteration: 1029 is: 2.800335645675659\n",
            "Loss in iteration: 1030 is: 2.8003339767456055\n",
            "Loss in iteration: 1031 is: 2.8003318309783936\n",
            "Loss in iteration: 1032 is: 2.8003296852111816\n",
            "Loss in iteration: 1033 is: 2.800328016281128\n",
            "Loss in iteration: 1034 is: 2.800326347351074\n",
            "Loss in iteration: 1035 is: 2.800323963165283\n",
            "Loss in iteration: 1036 is: 2.8003220558166504\n",
            "Loss in iteration: 1037 is: 2.8003201484680176\n",
            "Loss in iteration: 1038 is: 2.8003182411193848\n",
            "Loss in iteration: 1039 is: 2.8003158569335938\n",
            "Loss in iteration: 1040 is: 2.800313949584961\n",
            "Loss in iteration: 1041 is: 2.800312042236328\n",
            "Loss in iteration: 1042 is: 2.800309896469116\n",
            "Loss in iteration: 1043 is: 2.8003084659576416\n",
            "Loss in iteration: 1044 is: 2.8003058433532715\n",
            "Loss in iteration: 1045 is: 2.800304412841797\n",
            "Loss in iteration: 1046 is: 2.800302267074585\n",
            "Loss in iteration: 1047 is: 2.800300121307373\n",
            "Loss in iteration: 1048 is: 2.800297737121582\n",
            "Loss in iteration: 1049 is: 2.8002960681915283\n",
            "Loss in iteration: 1050 is: 2.8002943992614746\n",
            "Loss in iteration: 1051 is: 2.8002922534942627\n",
            "Loss in iteration: 1052 is: 2.800290107727051\n",
            "Loss in iteration: 1053 is: 2.800288200378418\n",
            "Loss in iteration: 1054 is: 2.800286054611206\n",
            "Loss in iteration: 1055 is: 2.8002841472625732\n",
            "Loss in iteration: 1056 is: 2.8002824783325195\n",
            "Loss in iteration: 1057 is: 2.8002800941467285\n",
            "Loss in iteration: 1058 is: 2.800278425216675\n",
            "Loss in iteration: 1059 is: 2.800276041030884\n",
            "Loss in iteration: 1060 is: 2.8002748489379883\n",
            "Loss in iteration: 1061 is: 2.800272226333618\n",
            "Loss in iteration: 1062 is: 2.8002700805664062\n",
            "Loss in iteration: 1063 is: 2.8002681732177734\n",
            "Loss in iteration: 1064 is: 2.8002662658691406\n",
            "Loss in iteration: 1065 is: 2.800264358520508\n",
            "Loss in iteration: 1066 is: 2.800262689590454\n",
            "Loss in iteration: 1067 is: 2.800260543823242\n",
            "Loss in iteration: 1068 is: 2.8002586364746094\n",
            "Loss in iteration: 1069 is: 2.8002562522888184\n",
            "Loss in iteration: 1070 is: 2.8002543449401855\n",
            "Loss in iteration: 1071 is: 2.800252914428711\n",
            "Loss in iteration: 1072 is: 2.800250768661499\n",
            "Loss in iteration: 1073 is: 2.800248622894287\n",
            "Loss in iteration: 1074 is: 2.8002467155456543\n",
            "Loss in iteration: 1075 is: 2.8002445697784424\n",
            "Loss in iteration: 1076 is: 2.8002424240112305\n",
            "Loss in iteration: 1077 is: 2.8002407550811768\n",
            "Loss in iteration: 1078 is: 2.800238609313965\n",
            "Loss in iteration: 1079 is: 2.800236701965332\n",
            "Loss in iteration: 1080 is: 2.8002350330352783\n",
            "Loss in iteration: 1081 is: 2.8002328872680664\n",
            "Loss in iteration: 1082 is: 2.8002307415008545\n",
            "Loss in iteration: 1083 is: 2.80022931098938\n",
            "Loss in iteration: 1084 is: 2.800227403640747\n",
            "Loss in iteration: 1085 is: 2.800224781036377\n",
            "Loss in iteration: 1086 is: 2.8002233505249023\n",
            "Loss in iteration: 1087 is: 2.8002212047576904\n",
            "Loss in iteration: 1088 is: 2.8002190589904785\n",
            "Loss in iteration: 1089 is: 2.800217390060425\n",
            "Loss in iteration: 1090 is: 2.800215244293213\n",
            "Loss in iteration: 1091 is: 2.800213098526001\n",
            "Loss in iteration: 1092 is: 2.8002114295959473\n",
            "Loss in iteration: 1093 is: 2.8002095222473145\n",
            "Loss in iteration: 1094 is: 2.8002076148986816\n",
            "Loss in iteration: 1095 is: 2.800205707550049\n",
            "Loss in iteration: 1096 is: 2.800203323364258\n",
            "Loss in iteration: 1097 is: 2.800201416015625\n",
            "Loss in iteration: 1098 is: 2.800199508666992\n",
            "Loss in iteration: 1099 is: 2.8001978397369385\n",
            "Loss in iteration: 1100 is: 2.8001952171325684\n",
            "Loss in iteration: 1101 is: 2.8001935482025146\n",
            "Loss in iteration: 1102 is: 2.8001914024353027\n",
            "Loss in iteration: 1103 is: 2.80018949508667\n",
            "Loss in iteration: 1104 is: 2.8001880645751953\n",
            "Loss in iteration: 1105 is: 2.800185441970825\n",
            "Loss in iteration: 1106 is: 2.8001842498779297\n",
            "Loss in iteration: 1107 is: 2.800182342529297\n",
            "Loss in iteration: 1108 is: 2.800179958343506\n",
            "Loss in iteration: 1109 is: 2.800178050994873\n",
            "Loss in iteration: 1110 is: 2.8001761436462402\n",
            "Loss in iteration: 1111 is: 2.8001742362976074\n",
            "Loss in iteration: 1112 is: 2.8001725673675537\n",
            "Loss in iteration: 1113 is: 2.800170421600342\n",
            "Loss in iteration: 1114 is: 2.800168752670288\n",
            "Loss in iteration: 1115 is: 2.800166368484497\n",
            "Loss in iteration: 1116 is: 2.8001646995544434\n",
            "Loss in iteration: 1117 is: 2.8001627922058105\n",
            "Loss in iteration: 1118 is: 2.8001606464385986\n",
            "Loss in iteration: 1119 is: 2.8001585006713867\n",
            "Loss in iteration: 1120 is: 2.800156831741333\n",
            "Loss in iteration: 1121 is: 2.800154685974121\n",
            "Loss in iteration: 1122 is: 2.8001527786254883\n",
            "Loss in iteration: 1123 is: 2.8001506328582764\n",
            "Loss in iteration: 1124 is: 2.8001492023468018\n",
            "Loss in iteration: 1125 is: 2.80014705657959\n",
            "Loss in iteration: 1126 is: 2.8001456260681152\n",
            "Loss in iteration: 1127 is: 2.8001434803009033\n",
            "Loss in iteration: 1128 is: 2.8001413345336914\n",
            "Loss in iteration: 1129 is: 2.8001391887664795\n",
            "Loss in iteration: 1130 is: 2.800137758255005\n",
            "Loss in iteration: 1131 is: 2.800135612487793\n",
            "Loss in iteration: 1132 is: 2.800133466720581\n",
            "Loss in iteration: 1133 is: 2.8001320362091064\n",
            "Loss in iteration: 1134 is: 2.8001296520233154\n",
            "Loss in iteration: 1135 is: 2.800128221511841\n",
            "Loss in iteration: 1136 is: 2.8001255989074707\n",
            "Loss in iteration: 1137 is: 2.800123929977417\n",
            "Loss in iteration: 1138 is: 2.800122022628784\n",
            "Loss in iteration: 1139 is: 2.8001201152801514\n",
            "Loss in iteration: 1140 is: 2.8001179695129395\n",
            "Loss in iteration: 1141 is: 2.8001163005828857\n",
            "Loss in iteration: 1142 is: 2.800114154815674\n",
            "Loss in iteration: 1143 is: 2.800112247467041\n",
            "Loss in iteration: 1144 is: 2.8001105785369873\n",
            "Loss in iteration: 1145 is: 2.8001084327697754\n",
            "Loss in iteration: 1146 is: 2.8001065254211426\n",
            "Loss in iteration: 1147 is: 2.8001043796539307\n",
            "Loss in iteration: 1148 is: 2.800102472305298\n",
            "Loss in iteration: 1149 is: 2.800100564956665\n",
            "Loss in iteration: 1150 is: 2.8000993728637695\n",
            "Loss in iteration: 1151 is: 2.8000969886779785\n",
            "Loss in iteration: 1152 is: 2.8000950813293457\n",
            "Loss in iteration: 1153 is: 2.800093173980713\n",
            "Loss in iteration: 1154 is: 2.800091028213501\n",
            "Loss in iteration: 1155 is: 2.8000893592834473\n",
            "Loss in iteration: 1156 is: 2.8000872135162354\n",
            "Loss in iteration: 1157 is: 2.8000857830047607\n",
            "Loss in iteration: 1158 is: 2.8000833988189697\n",
            "Loss in iteration: 1159 is: 2.800081729888916\n",
            "Loss in iteration: 1160 is: 2.800079584121704\n",
            "Loss in iteration: 1161 is: 2.8000779151916504\n",
            "Loss in iteration: 1162 is: 2.8000760078430176\n",
            "Loss in iteration: 1163 is: 2.8000741004943848\n",
            "Loss in iteration: 1164 is: 2.800072193145752\n",
            "Loss in iteration: 1165 is: 2.8000705242156982\n",
            "Loss in iteration: 1166 is: 2.800067901611328\n",
            "Loss in iteration: 1167 is: 2.8000664710998535\n",
            "Loss in iteration: 1168 is: 2.8000643253326416\n",
            "Loss in iteration: 1169 is: 2.800062656402588\n",
            "Loss in iteration: 1170 is: 2.800060987472534\n",
            "Loss in iteration: 1171 is: 2.8000588417053223\n",
            "Loss in iteration: 1172 is: 2.8000571727752686\n",
            "Loss in iteration: 1173 is: 2.8000550270080566\n",
            "Loss in iteration: 1174 is: 2.800053358078003\n",
            "Loss in iteration: 1175 is: 2.80005145072937\n",
            "Loss in iteration: 1176 is: 2.8000495433807373\n",
            "Loss in iteration: 1177 is: 2.8000473976135254\n",
            "Loss in iteration: 1178 is: 2.8000454902648926\n",
            "Loss in iteration: 1179 is: 2.8000431060791016\n",
            "Loss in iteration: 1180 is: 2.800041675567627\n",
            "Loss in iteration: 1181 is: 2.8000402450561523\n",
            "Loss in iteration: 1182 is: 2.8000378608703613\n",
            "Loss in iteration: 1183 is: 2.8000357151031494\n",
            "Loss in iteration: 1184 is: 2.800034284591675\n",
            "Loss in iteration: 1185 is: 2.800032377243042\n",
            "Loss in iteration: 1186 is: 2.80003023147583\n",
            "Loss in iteration: 1187 is: 2.8000283241271973\n",
            "Loss in iteration: 1188 is: 2.8000264167785645\n",
            "Loss in iteration: 1189 is: 2.8000245094299316\n",
            "Loss in iteration: 1190 is: 2.8000223636627197\n",
            "Loss in iteration: 1191 is: 2.800020933151245\n",
            "Loss in iteration: 1192 is: 2.800018548965454\n",
            "Loss in iteration: 1193 is: 2.8000168800354004\n",
            "Loss in iteration: 1194 is: 2.8000152111053467\n",
            "Loss in iteration: 1195 is: 2.800013542175293\n",
            "Loss in iteration: 1196 is: 2.80001163482666\n",
            "Loss in iteration: 1197 is: 2.800009250640869\n",
            "Loss in iteration: 1198 is: 2.8000075817108154\n",
            "Loss in iteration: 1199 is: 2.8000056743621826\n",
            "Loss in iteration: 1200 is: 2.80000376701355\n",
            "Loss in iteration: 1201 is: 2.800002098083496\n",
            "Loss in iteration: 1202 is: 2.799999952316284\n",
            "Loss in iteration: 1203 is: 2.7999978065490723\n",
            "Loss in iteration: 1204 is: 2.7999963760375977\n",
            "Loss in iteration: 1205 is: 2.799994468688965\n",
            "Loss in iteration: 1206 is: 2.799992561340332\n",
            "Loss in iteration: 1207 is: 2.7999908924102783\n",
            "Loss in iteration: 1208 is: 2.7999887466430664\n",
            "Loss in iteration: 1209 is: 2.7999868392944336\n",
            "Loss in iteration: 1210 is: 2.79998517036438\n",
            "Loss in iteration: 1211 is: 2.799983501434326\n",
            "Loss in iteration: 1212 is: 2.799980878829956\n",
            "Loss in iteration: 1213 is: 2.7999794483184814\n",
            "Loss in iteration: 1214 is: 2.7999773025512695\n",
            "Loss in iteration: 1215 is: 2.799975872039795\n",
            "Loss in iteration: 1216 is: 2.799973964691162\n",
            "Loss in iteration: 1217 is: 2.799971580505371\n",
            "Loss in iteration: 1218 is: 2.7999699115753174\n",
            "Loss in iteration: 1219 is: 2.7999682426452637\n",
            "Loss in iteration: 1220 is: 2.799966335296631\n",
            "Loss in iteration: 1221 is: 2.799964189529419\n",
            "Loss in iteration: 1222 is: 2.7999629974365234\n",
            "Loss in iteration: 1223 is: 2.7999608516693115\n",
            "Loss in iteration: 1224 is: 2.7999589443206787\n",
            "Loss in iteration: 1225 is: 2.799957275390625\n",
            "Loss in iteration: 1226 is: 2.799955368041992\n",
            "Loss in iteration: 1227 is: 2.7999532222747803\n",
            "Loss in iteration: 1228 is: 2.7999515533447266\n",
            "Loss in iteration: 1229 is: 2.7999496459960938\n",
            "Loss in iteration: 1230 is: 2.799947738647461\n",
            "Loss in iteration: 1231 is: 2.799945592880249\n",
            "Loss in iteration: 1232 is: 2.799943685531616\n",
            "Loss in iteration: 1233 is: 2.7999422550201416\n",
            "Loss in iteration: 1234 is: 2.7999398708343506\n",
            "Loss in iteration: 1235 is: 2.799938440322876\n",
            "Loss in iteration: 1236 is: 2.799936532974243\n",
            "Loss in iteration: 1237 is: 2.7999351024627686\n",
            "Loss in iteration: 1238 is: 2.7999329566955566\n",
            "Loss in iteration: 1239 is: 2.7999308109283447\n",
            "Loss in iteration: 1240 is: 2.799928665161133\n",
            "Loss in iteration: 1241 is: 2.7999267578125\n",
            "Loss in iteration: 1242 is: 2.799924850463867\n",
            "Loss in iteration: 1243 is: 2.7999236583709717\n",
            "Loss in iteration: 1244 is: 2.7999212741851807\n",
            "Loss in iteration: 1245 is: 2.799919366836548\n",
            "Loss in iteration: 1246 is: 2.799917697906494\n",
            "Loss in iteration: 1247 is: 2.7999162673950195\n",
            "Loss in iteration: 1248 is: 2.7999141216278076\n",
            "Loss in iteration: 1249 is: 2.799912452697754\n",
            "Loss in iteration: 1250 is: 2.799910545349121\n",
            "Loss in iteration: 1251 is: 2.799908399581909\n",
            "Loss in iteration: 1252 is: 2.7999069690704346\n",
            "Loss in iteration: 1253 is: 2.7999048233032227\n",
            "Loss in iteration: 1254 is: 2.79990291595459\n",
            "Loss in iteration: 1255 is: 2.799901008605957\n",
            "Loss in iteration: 1256 is: 2.7998995780944824\n",
            "Loss in iteration: 1257 is: 2.7998974323272705\n",
            "Loss in iteration: 1258 is: 2.7998952865600586\n",
            "Loss in iteration: 1259 is: 2.799893856048584\n",
            "Loss in iteration: 1260 is: 2.799891948699951\n",
            "Loss in iteration: 1261 is: 2.7998900413513184\n",
            "Loss in iteration: 1262 is: 2.7998881340026855\n",
            "Loss in iteration: 1263 is: 2.7998862266540527\n",
            "Loss in iteration: 1264 is: 2.799884796142578\n",
            "Loss in iteration: 1265 is: 2.799882650375366\n",
            "Loss in iteration: 1266 is: 2.7998809814453125\n",
            "Loss in iteration: 1267 is: 2.799879312515259\n",
            "Loss in iteration: 1268 is: 2.799877643585205\n",
            "Loss in iteration: 1269 is: 2.799875497817993\n",
            "Loss in iteration: 1270 is: 2.7998735904693604\n",
            "Loss in iteration: 1271 is: 2.7998719215393066\n",
            "Loss in iteration: 1272 is: 2.799870014190674\n",
            "Loss in iteration: 1273 is: 2.799868106842041\n",
            "Loss in iteration: 1274 is: 2.799866199493408\n",
            "Loss in iteration: 1275 is: 2.7998642921447754\n",
            "Loss in iteration: 1276 is: 2.7998626232147217\n",
            "Loss in iteration: 1277 is: 2.7998604774475098\n",
            "Loss in iteration: 1278 is: 2.799859046936035\n",
            "Loss in iteration: 1279 is: 2.7998569011688232\n",
            "Loss in iteration: 1280 is: 2.7998552322387695\n",
            "Loss in iteration: 1281 is: 2.799853801727295\n",
            "Loss in iteration: 1282 is: 2.799851655960083\n",
            "Loss in iteration: 1283 is: 2.799849271774292\n",
            "Loss in iteration: 1284 is: 2.7998478412628174\n",
            "Loss in iteration: 1285 is: 2.7998461723327637\n",
            "Loss in iteration: 1286 is: 2.7998440265655518\n",
            "Loss in iteration: 1287 is: 2.799842357635498\n",
            "Loss in iteration: 1288 is: 2.7998409271240234\n",
            "Loss in iteration: 1289 is: 2.7998387813568115\n",
            "Loss in iteration: 1290 is: 2.7998368740081787\n",
            "Loss in iteration: 1291 is: 2.7998344898223877\n",
            "Loss in iteration: 1292 is: 2.799832820892334\n",
            "Loss in iteration: 1293 is: 2.7998316287994385\n",
            "Loss in iteration: 1294 is: 2.7998297214508057\n",
            "Loss in iteration: 1295 is: 2.7998273372650146\n",
            "Loss in iteration: 1296 is: 2.799825668334961\n",
            "Loss in iteration: 1297 is: 2.7998242378234863\n",
            "Loss in iteration: 1298 is: 2.7998223304748535\n",
            "Loss in iteration: 1299 is: 2.7998206615448\n",
            "Loss in iteration: 1300 is: 2.799818515777588\n",
            "Loss in iteration: 1301 is: 2.799816846847534\n",
            "Loss in iteration: 1302 is: 2.7998149394989014\n",
            "Loss in iteration: 1303 is: 2.7998130321502686\n",
            "Loss in iteration: 1304 is: 2.799811363220215\n",
            "Loss in iteration: 1305 is: 2.799809217453003\n",
            "Loss in iteration: 1306 is: 2.7998080253601074\n",
            "Loss in iteration: 1307 is: 2.7998061180114746\n",
            "Loss in iteration: 1308 is: 2.7998039722442627\n",
            "Loss in iteration: 1309 is: 2.799802780151367\n",
            "Loss in iteration: 1310 is: 2.7998008728027344\n",
            "Loss in iteration: 1311 is: 2.7997987270355225\n",
            "Loss in iteration: 1312 is: 2.7997968196868896\n",
            "Loss in iteration: 1313 is: 2.799795150756836\n",
            "Loss in iteration: 1314 is: 2.799793243408203\n",
            "Loss in iteration: 1315 is: 2.7997913360595703\n",
            "Loss in iteration: 1316 is: 2.7997896671295166\n",
            "Loss in iteration: 1317 is: 2.799787998199463\n",
            "Loss in iteration: 1318 is: 2.79978609085083\n",
            "Loss in iteration: 1319 is: 2.799783945083618\n",
            "Loss in iteration: 1320 is: 2.7997822761535645\n",
            "Loss in iteration: 1321 is: 2.7997803688049316\n",
            "Loss in iteration: 1322 is: 2.799778699874878\n",
            "Loss in iteration: 1323 is: 2.7997772693634033\n",
            "Loss in iteration: 1324 is: 2.7997751235961914\n",
            "Loss in iteration: 1325 is: 2.7997734546661377\n",
            "Loss in iteration: 1326 is: 2.799771547317505\n",
            "Loss in iteration: 1327 is: 2.799769639968872\n",
            "Loss in iteration: 1328 is: 2.7997682094573975\n",
            "Loss in iteration: 1329 is: 2.7997660636901855\n",
            "Loss in iteration: 1330 is: 2.7997639179229736\n",
            "Loss in iteration: 1331 is: 2.799762487411499\n",
            "Loss in iteration: 1332 is: 2.7997610569000244\n",
            "Loss in iteration: 1333 is: 2.7997589111328125\n",
            "Loss in iteration: 1334 is: 2.7997570037841797\n",
            "Loss in iteration: 1335 is: 2.799755573272705\n",
            "Loss in iteration: 1336 is: 2.7997536659240723\n",
            "Loss in iteration: 1337 is: 2.7997517585754395\n",
            "Loss in iteration: 1338 is: 2.7997498512268066\n",
            "Loss in iteration: 1339 is: 2.799748182296753\n",
            "Loss in iteration: 1340 is: 2.79974627494812\n",
            "Loss in iteration: 1341 is: 2.7997443675994873\n",
            "Loss in iteration: 1342 is: 2.7997424602508545\n",
            "Loss in iteration: 1343 is: 2.79974102973938\n",
            "Loss in iteration: 1344 is: 2.799739360809326\n",
            "Loss in iteration: 1345 is: 2.7997374534606934\n",
            "Loss in iteration: 1346 is: 2.7997353076934814\n",
            "Loss in iteration: 1347 is: 2.7997336387634277\n",
            "Loss in iteration: 1348 is: 2.799732208251953\n",
            "Loss in iteration: 1349 is: 2.7997303009033203\n",
            "Loss in iteration: 1350 is: 2.7997286319732666\n",
            "Loss in iteration: 1351 is: 2.799727201461792\n",
            "Loss in iteration: 1352 is: 2.79972505569458\n",
            "Loss in iteration: 1353 is: 2.799722909927368\n",
            "Loss in iteration: 1354 is: 2.7997217178344727\n",
            "Loss in iteration: 1355 is: 2.7997195720672607\n",
            "Loss in iteration: 1356 is: 2.799717903137207\n",
            "Loss in iteration: 1357 is: 2.7997162342071533\n",
            "Loss in iteration: 1358 is: 2.7997143268585205\n",
            "Loss in iteration: 1359 is: 2.799712657928467\n",
            "Loss in iteration: 1360 is: 2.799710512161255\n",
            "Loss in iteration: 1361 is: 2.7997093200683594\n",
            "Loss in iteration: 1362 is: 2.7997071743011475\n",
            "Loss in iteration: 1363 is: 2.7997055053710938\n",
            "Loss in iteration: 1364 is: 2.79970383644104\n",
            "Loss in iteration: 1365 is: 2.799701690673828\n",
            "Loss in iteration: 1366 is: 2.7997000217437744\n",
            "Loss in iteration: 1367 is: 2.7996983528137207\n",
            "Loss in iteration: 1368 is: 2.799696207046509\n",
            "Loss in iteration: 1369 is: 2.799694538116455\n",
            "Loss in iteration: 1370 is: 2.7996933460235596\n",
            "Loss in iteration: 1371 is: 2.7996912002563477\n",
            "Loss in iteration: 1372 is: 2.799689769744873\n",
            "Loss in iteration: 1373 is: 2.7996878623962402\n",
            "Loss in iteration: 1374 is: 2.7996859550476074\n",
            "Loss in iteration: 1375 is: 2.7996840476989746\n",
            "Loss in iteration: 1376 is: 2.7996819019317627\n",
            "Loss in iteration: 1377 is: 2.799680709838867\n",
            "Loss in iteration: 1378 is: 2.7996790409088135\n",
            "Loss in iteration: 1379 is: 2.7996768951416016\n",
            "Loss in iteration: 1380 is: 2.7996749877929688\n",
            "Loss in iteration: 1381 is: 2.799673318862915\n",
            "Loss in iteration: 1382 is: 2.7996716499328613\n",
            "Loss in iteration: 1383 is: 2.7996702194213867\n",
            "Loss in iteration: 1384 is: 2.799668312072754\n",
            "Loss in iteration: 1385 is: 2.799666404724121\n",
            "Loss in iteration: 1386 is: 2.7996644973754883\n",
            "Loss in iteration: 1387 is: 2.7996625900268555\n",
            "Loss in iteration: 1388 is: 2.79966139793396\n",
            "Loss in iteration: 1389 is: 2.799659013748169\n",
            "Loss in iteration: 1390 is: 2.7996573448181152\n",
            "Loss in iteration: 1391 is: 2.7996556758880615\n",
            "Loss in iteration: 1392 is: 2.7996537685394287\n",
            "Loss in iteration: 1393 is: 2.799652099609375\n",
            "Loss in iteration: 1394 is: 2.799650192260742\n",
            "Loss in iteration: 1395 is: 2.7996487617492676\n",
            "Loss in iteration: 1396 is: 2.7996468544006348\n",
            "Loss in iteration: 1397 is: 2.79964542388916\n",
            "Loss in iteration: 1398 is: 2.7996435165405273\n",
            "Loss in iteration: 1399 is: 2.7996416091918945\n",
            "Loss in iteration: 1400 is: 2.79964017868042\n",
            "Loss in iteration: 1401 is: 2.799638032913208\n",
            "Loss in iteration: 1402 is: 2.799636125564575\n",
            "Loss in iteration: 1403 is: 2.7996344566345215\n",
            "Loss in iteration: 1404 is: 2.799633026123047\n",
            "Loss in iteration: 1405 is: 2.799630880355835\n",
            "Loss in iteration: 1406 is: 2.7996292114257812\n",
            "Loss in iteration: 1407 is: 2.7996273040771484\n",
            "Loss in iteration: 1408 is: 2.7996256351470947\n",
            "Loss in iteration: 1409 is: 2.79962420463562\n",
            "Loss in iteration: 1410 is: 2.7996222972869873\n",
            "Loss in iteration: 1411 is: 2.7996208667755127\n",
            "Loss in iteration: 1412 is: 2.79961895942688\n",
            "Loss in iteration: 1413 is: 2.799617290496826\n",
            "Loss in iteration: 1414 is: 2.7996153831481934\n",
            "Loss in iteration: 1415 is: 2.7996139526367188\n",
            "Loss in iteration: 1416 is: 2.799612283706665\n",
            "Loss in iteration: 1417 is: 2.799610137939453\n",
            "Loss in iteration: 1418 is: 2.7996089458465576\n",
            "Loss in iteration: 1419 is: 2.799607038497925\n",
            "Loss in iteration: 1420 is: 2.799604892730713\n",
            "Loss in iteration: 1421 is: 2.799603223800659\n",
            "Loss in iteration: 1422 is: 2.7996017932891846\n",
            "Loss in iteration: 1423 is: 2.7995998859405518\n",
            "Loss in iteration: 1424 is: 2.799597978591919\n",
            "layer_1.weight tensor([[ 1.3622e+01,  6.2305e+00,  1.3659e+01,  1.3659e+01, -6.2907e+00,\n",
            "          1.3622e+01],\n",
            "        [ 6.4269e-03,  7.2803e-03, -5.1345e+00, -5.1345e+00,  7.4187e-03,\n",
            "          6.4269e-03]])\n",
            "layer_1.bias tensor([-4.6353, 10.6479])\n",
            "layer_2.weight tensor([[-3.8583, -5.0841]])\n",
            "layer_2.bias tensor([5.7934])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we plot the loss which shows that for the chosen loss function (i.e. total error), model seems not to be fully stabilized after 30 epochs, even if the marginal reduction in the loss seems decreasing at the increase of the optimization steps."
      ],
      "metadata": {
        "id": "fuLVDe8TGXbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize loss during training\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Mirror symmetry detection loss\")\n",
        "plt.plot(train_loss)\n",
        "plt.xlabel(\"Optimization step\")\n",
        "plt.ylabel(\"TE Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9hFdosY82RpN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f912ea93-27c3-4052-de14-0bcd561a6421"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusions\n",
        "---\n",
        "The inspection of the final weights does not seem to lead to the same results of the original paper (i.e. weights following the 1:2:4 ratio in a symmetric pattern, negative biases for hidden units, positive for the output unit).\n",
        "Indeed we notice a quite significant symmetry in the weights we obtained, moreover also our output bias is positive, but the biases in the hidden layer have both negative and positive sign and it doesn't seem to hold the 1:2:4 ratio in the resulting weights.\n",
        "Possible ways to improve our results may be to re-think the way we created our dataset, investigating if we may have misinterpreted some elements of the original paper. Another improvement may be to increase the total number of epochs, to try to refine the resulting model, possibly reducing losses. A third direction which may be worth exploring is changing the optimizer, to see if alternatives such as Adam, RMSprop or Adagrad lead to results more in line with the original work. Finally, a word of caution even on the pseudo-random seed adopted, as of course the stochasticity introduced in our experiment may lead to elements which impair the direct comparison with the original work. "
      ],
      "metadata": {
        "id": "WB9tlvDgG_mc"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}