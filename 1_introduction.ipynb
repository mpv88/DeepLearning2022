{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5C6vCuBhjjGm"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lorenzobasile/DeepLearning2022/blob/main/1_introduction.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J-xj0WE5-Hp"
   },
   "source": [
    "\n",
    "# Introduction to Labs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onJ4CutWhFjA"
   },
   "source": [
    "Teaching assistant: Lorenzo Basile (lore.basile@outlook.com), PhD student@UniTS\n",
    "\n",
    "Please, do use the email for any doubt and clarification about labs and assignments!\n",
    "\n",
    "Labs will be largely based on the notebooks by Marco Zullich (https://github.com/ansuini/DSSC_DL_2022/)\n",
    "\n",
    "\n",
    "## Tentative structure of the course\n",
    "\n",
    "\n",
    "*   Lectures\n",
    "*   Laboratories (probably 7 sessions)\n",
    "*   Lectures by experts on selected topics (sometimes on Fridays):\n",
    "    \n",
    "    *   Graph Neural Networks \n",
    "    *   Visual Cortex and DL\n",
    "    *   Language Models and applications in Biology\n",
    "    *   Diffusion Models\n",
    "\n",
    "3 labs will end with the presentation of a homework, with delivery date approximately after 2 weeks. Timely delivery of the homework is not compulsory to pass the exam but will result in a lighter final exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zieSJTheEHb"
   },
   "source": [
    "## Computational resources\n",
    "\n",
    "We will not run particularly heavy experiments during the labs, so for most parts you should be able to reproduce the experiments on the CPU of your personal laptop or desktop. However, to avoid issues with library versions and to avoid installing any package (and to take advantage of some hardware acceleration from time to time), we will be running the labs on Google Colab, a service that provides free access GPUs.\n",
    "\n",
    "For your final projects, since you're likely to be doing something a bit heavier than what we see during the labs (and to gain some more experience with HPC facilities), it is a good idea to switch to the Orfeo cluster of AREA Science Park.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD1NrDYig7Ev"
   },
   "source": [
    "# Introduction to Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxhBDiM7hwnZ"
   },
   "source": [
    "Colab is a free service provided by Google for ML research. It is based on Jupyter notebooks that run on a remote server, and it provides free (but limited time) GPU acceleration.\n",
    "\n",
    "To enable GPU or TPU acceleration just go to `Runtime>Change runtime type` and choose from the menu. Please note that GPU usage is limited in time, so avoid requesting one if you do not really need it.\n",
    "\n",
    "Inside a code cell you can use `!` to run shell commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yobTsVNBsNZo"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi    # if you enable GPU acceleration, this command returns information on the GPU\n",
    "!pip install torch==1.11.0    # just an example, torch is already installed in Colab\n",
    "!sudo apt-get install gcc    # you can also run sudo commands\n",
    "!wget https://roboti.us/download/mujoco200_linux.zip    # and download data to a temporary memory\n",
    "!git clone git@github.com:lorenzobasile/DeepLearning2022.git    # just another example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efPBqlRFkWaO"
   },
   "source": [
    "## Colab file system\n",
    "\n",
    "By default, Colab accesses a volatile memory that is erased as soon as your process terminates, but you can interface it with your personal Google Drive to read and write data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nD70La0jmV9S"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKgtLsgU3Eaq"
   },
   "outputs": [],
   "source": [
    "# note that if you want this command to be permanent you need to use the magic % instead of !\n",
    "%cd drive/MyDrive/DeepLearning2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZdJ_J8ymbVm"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BEwIy23eI_6"
   },
   "source": [
    "# Brief guide to Orfeo cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9Gt3IJ35fKf"
   },
   "source": [
    "For your projects, you may want to run slightly heavier experiments, that require a GPU for more time than Colab allows you to have for free. To do so, you will be provided access to the Orfeo cluster of AREA Science Park (more details on that later).\n",
    "\n",
    "Orfeo is very straightforward to use, as it does not differ from a standard linux shell for most tasks. There are just a couple of commands to remember:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3IFU32WeJuv"
   },
   "outputs": [],
   "source": [
    "! ssh username@ct1-005.area.trieste.it    # to login\n",
    "! qsub -q dssc_gpu -l nodes=N:ppn=M,walltime=HH:MM:SS -I    # to request access to a GPU computational node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jy4UsU666vMp"
   },
   "source": [
    "Please note that the second command is asking for `M` cores per node on `N` computational nodes, for the time specified by the walltime argument. **Always remember to ask for a computational node before running any code**.\n",
    "\n",
    "The flag `-I` means that we want to run interactively (i.e. we want to interact with the computational node through a linux shell). This is not the only possibility, and usually it is advisable to send a job by means of a bash script. To do so, just replace `-I` with the path to the script (for example `script.sh`).\n",
    "\n",
    "Most libraries that you will need are preinstalled on Orfeo through conda, which can be loaded using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! module load conda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you send a bash script, at the end you will see two files in the same folder of your script, named `script.sh.o<JOB NUMBER>`and `script.sh.e<JOB NUMBER>`, containing respectively the `stdout` and `stderr` of your job.\n",
    "\n",
    "You can track (and terminate if needed) you running jobs by using the commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_lsBj1q7S_W"
   },
   "outputs": [],
   "source": [
    "! qstat -u username    # this command will return a list of the current processes you are running\n",
    "! qdel <JOB NUMBER>    # to kill the selected process, the JOB NUMBER can be obtained reading the qstat information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDmeXyfm8Sjj"
   },
   "source": [
    "# Getting started with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJIabHGPAyjA"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7RMvAS7_FZu"
   },
   "source": [
    "## What is PyTorch?\n",
    "\n",
    "PyTorch (or informally torch) is a Python library specifically built for Deep Learning, that comes with a series of very useful functionalities that make it one of the most popular tools for DL research and application.\n",
    "\n",
    "Namely, it has many built-in features and modules useful for DL, tensor arithmetic and automatic differentiation features, and it allows for easy GPU acceleration through CUDA.\n",
    "\n",
    "Another famous library for DL you may have heard of is TensorFlow, which also has a more user-friendly interface called Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Whh-6LIWBaeO"
   },
   "source": [
    "## Basic operation with Tensors\n",
    "\n",
    "The main building block of PyTorch is the `Tensor` class. A torch `Tensor` is the equivalent of NumPy `ndarray` and most of the functionalities are the same as in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpD4ME148gl3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=torch.tensor([[1,2,3],[4,5,6]])\n",
    "y=np.array([[1,2,3],[4,5,6]])\n",
    "\n",
    "print(\"X:\", x)\n",
    "print(\"Y:\", y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qtg8opJNFAUL"
   },
   "source": [
    "Basic NumPy array features exist for torch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6znBc2hDtDh"
   },
   "outputs": [],
   "source": [
    "x.shape, y.shape, x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOBslFT5FCRz"
   },
   "outputs": [],
   "source": [
    "x.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRewW0F_G7Q0"
   },
   "source": [
    "Note that you can build a tensor through the constructor `torch.Tensor`. In this case, since `torch.Tensor` is an alias for `torch.FloatTensor`, the tensor you create will have type `torch.float32`.\n",
    "\n",
    "You can convert the dtype of a tensor by using the functions `float()`, `int()` etc.\n",
    "\n",
    "More info on data types [here](https://pytorch.org/docs/stable/tensors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6LBpF5FHSgD"
   },
   "outputs": [],
   "source": [
    "x=torch.Tensor([[1,2,3], [4,5,6]])\n",
    "print(\"Dtype of X:\", x.dtype)\n",
    "x=x.int()\n",
    "print(\"Dtype of X:\", x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bqa2Dz8dGFnI"
   },
   "source": [
    "And you can create random tensors just like you create random arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UoOMPGuFMf2"
   },
   "outputs": [],
   "source": [
    "x=torch.rand(2,3,2)    # you can also use a list or a tuple for the dimensions\n",
    "y=np.random.rand(2,3,2)\n",
    "print(\"X:\", x)\n",
    "print(\"Y:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgXgfVm7MctR"
   },
   "source": [
    "You can easily compute statistics of tensors (such as the sum, mean, max, min, std... of the elements) by either using the methods of the `Tensor` class or using the basic torch functions and using your tensor as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gjgFjspFT-3"
   },
   "outputs": [],
   "source": [
    "x.sum(), torch.sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "px8JQCXlMuE-"
   },
   "outputs": [],
   "source": [
    "x.mean(), torch.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsIvuPJ1M9E0"
   },
   "outputs": [],
   "source": [
    "x.argmin(), torch.argmin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDlly60nNDuk"
   },
   "source": [
    "It is sometimes very useful to specify one or more dimensions to reduce (along which you want to perform your operations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2taOpNVK5Fk"
   },
   "outputs": [],
   "source": [
    "print(x)\n",
    "x.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0yfA2jqMFo4"
   },
   "outputs": [],
   "source": [
    "x.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pArlxdjJNXs1"
   },
   "outputs": [],
   "source": [
    "x.sum(dim=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfL7l01dOZ89"
   },
   "source": [
    "Tensor slicing works exactly like in NumPy, by means of square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3g-ytVvbNtmH"
   },
   "outputs": [],
   "source": [
    "x[0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkBPG4UCOZJI"
   },
   "outputs": [],
   "source": [
    "x[0,1:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gCnGoHXPGM-"
   },
   "outputs": [],
   "source": [
    "x[:,::2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2Zn_AKJPsYf"
   },
   "source": [
    "## Linear algebra and tensor reshaping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6LQprgNQmGx"
   },
   "source": [
    "An operation we will frequently perform in Deep Learning (though often under the hood) is matrix multiplication. In torch, it can be done in many equivalent ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDXKKSCvPueK"
   },
   "outputs": [],
   "source": [
    "x=torch.rand(4,5)\n",
    "y=x.T    # matrix transposition\n",
    "\n",
    "print(x@y)\n",
    "print(x.matmul(y))\n",
    "print(torch.matmul(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUPJEi4LRC6F"
   },
   "source": [
    "Please note that the operator for matrix multiplication is `@`, not `*`, which indicates the Hadamard (element-wise) product instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EduEPVkcPw9M"
   },
   "outputs": [],
   "source": [
    "x*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7p8e9LpSRgqv"
   },
   "source": [
    "Multiplying a matrix by itself is obviously equivalent to computing its power, and it can be done also by running one of the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6wkhv7NRaLl"
   },
   "outputs": [],
   "source": [
    "torch.pow(x,2), x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwqdFzYhSf8v"
   },
   "source": [
    "As in NumPy, there exists a `dot` function to compute the scalar product between vectors. Note that differently from NumPy, in torch this is **not** equivalent to matrix multiplication, as it is intended to work only with 1D vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nB7ZAzO4RnCn"
   },
   "outputs": [],
   "source": [
    "v1=x[:,1]\n",
    "v2=x[:,2]\n",
    "print(v1.shape, v2.shape)\n",
    "\n",
    "print(v1.dot(v2))    # in the case of 1D vectors, there is no difference between row and column vectors\n",
    "print(v1.matmul(v2))\n",
    "print(v1@v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yryQXyVoTVUJ"
   },
   "source": [
    "If you want to do something fancier with two vectors, like multiplying a column by a row to obtain a matrix, you need to switch to 2D vectors by reshaping them.\n",
    "\n",
    "When you reshape a tensor, you can leave one dimension unspecified (using -1), as it can be inferred automatically by torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1HHk1CYR5Kk"
   },
   "outputs": [],
   "source": [
    "v1=v1.reshape(-1,1)    # column vector\n",
    "v2=v2.reshape(1,-1)    # row vector\n",
    "\n",
    "print(v1.shape, v2.shape)\n",
    "print(v1@v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o76-vuJYSEqI"
   },
   "outputs": [],
   "source": [
    "print(v1.dot(v2))    # this doesn't work! dot works only on 1D tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kH-zNQceYDMq"
   },
   "source": [
    "Changing the shape of a tensor is a crucial operation in DL. To have an idea of its application, just think of RGB images, commonly used in Computer Vision.\n",
    "\n",
    "These are $3\\times H\\times W$ tensors, where H and W stand for height and width of the image (in number of pixels). It is often needed to regard an image as a linearized (flattened array of pixels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1mkOCIIUhMM"
   },
   "outputs": [],
   "source": [
    "img=torch.rand(3,8,8)\n",
    "img.reshape(3,64)    # note that reshaping is not in place, so this call does not change the actual shape of img\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHH3StilnKWc"
   },
   "source": [
    "Very often (for instance when you have to pass an image to `matplotlib` for visualization), you need to change the shape of an image to $H\\times W \\times 3$. You may be tempted to do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhzYnjRrnhuB"
   },
   "outputs": [],
   "source": [
    "new_img=img.reshape(8,8,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4tEQZnjnoMn"
   },
   "source": [
    "This piece of code runs seamlessly, since the dimensions are consistent with the original ones. However, it will not produce the expected behaviour.\n",
    "\n",
    "In fact, `reshape` only modifies the shape of a tensor, without touching the way data are stored in memory, meaning that you would end up mixing data from different dimensions.\n",
    "\n",
    "The right way to change the order of dimensions is to use `permute`, which accepts as argument the ordering of dimensions that you desire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rQYWvChJ3BD"
   },
   "outputs": [],
   "source": [
    "new_img=img.permute(1,2,0)\n",
    "print(new_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBdhQQc0ckAA"
   },
   "source": [
    "# Building Machine Learning models with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDp7I1jqc1ZT"
   },
   "source": [
    "## Linear regression\n",
    "\n",
    "By using all the pieces we've seen till now, we can build our first ML model using PyTorch: a linear regressor, whose model is:\n",
    "\n",
    "$$\n",
    "y = XW + b\n",
    "$$\n",
    "\n",
    "which can also be simplified as:\n",
    "\n",
    "$$\n",
    "y = XW\n",
    "$$\n",
    "\n",
    "if we incorporate the bias $b$ inside $W$ and add to the $X$ a column of ones to the right.\n",
    "\n",
    "\n",
    "We start by creating our data. We randomly sample $X$ as a $N\\times P$ tensor, meaning that we have 1000 datapoints and 100 features and compute $y$ as:\n",
    "$$\n",
    "y=XM+\\mathcal{N}(0,I)\n",
    "$$\n",
    "where $M$ is a randomly drawn projection vector (shape $P\\times 1$, same as our weights).\n",
    "We are adding some iid gaussian noise on the $y$ to avoid the interpolation regime, in which we could be fitting our data perfectly using a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rih2SvjkcyGk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "N=1000\n",
    "P=100\n",
    "X=torch.rand(N,P)\n",
    "M=torch.rand(P,1)\n",
    "y=X@M+torch.normal(torch.zeros(N,1),torch.ones(N,1))\n",
    "\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.makedirs(\"./data\")\n",
    "\n",
    "torch.save(X, \"./data/X_reg.pt\")\n",
    "torch.save(y, \"./data/y_reg.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzqfSPVANPsK"
   },
   "source": [
    "We can add a column of ones to $X$ to include the bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inmRPdHDNWNE"
   },
   "outputs": [],
   "source": [
    "X=torch.cat([X, torch.ones(N,1)], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhSx42OFK6YK"
   },
   "source": [
    "The regression can be fit with classical statistical methods such as Ordinary Least Squares, and the optimal $W$ has the form:\n",
    "\n",
    "$$\n",
    "W^*=(X^TX)^{-1}X^Ty\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjHK46Taf6EP"
   },
   "outputs": [],
   "source": [
    "W_star = ((X.T @ X).inverse()) @ X.T @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WONzQvFMw5d"
   },
   "source": [
    "To assess the quality of this fit we can evaluate the Mean Squared Error (MSE) between the original $y$ and the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jJG9dxIgTY8"
   },
   "outputs": [],
   "source": [
    "torch.nn.functional.mse_loss(X@W_star, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXdU0Hn0R387"
   },
   "source": [
    "## The same linear model, but in PyTorch style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8T5AHAXPaR3"
   },
   "source": [
    "A linear model like the one we saw before is nothing more than an artificial neuron with no activation function.\n",
    "\n",
    "We will now be exploring the second chunk of PT functionalities, namely the built-in structures and routines supporting the creation of ML models.\n",
    "\n",
    "We can create the same model we have seen before using torch built-in structures, so we start to see them right away.\n",
    "\n",
    "Usually, a torch model is a `class` inheriting from `torch.nn.Module`. Inside this class, we'll define two methods:\n",
    "* the constructor (`__init__`) in which we define the building blocks of our model as class variables;\n",
    "* the `forward` method, which specifies how the data fed into the model needs to be processed in order to produce the output.\n",
    "\n",
    "Note for those who already know something about NNs: we don't need to define `backward` methods since we're constructing our model with built-in PT building blocks. PyTorch automatically creates a `backward` routine based upon the `forward` method.\n",
    "\n",
    "Our model only has one building block (layer) which is a `Linear` layer.\n",
    "We need to specify the size of the input (i.e. the coefficients $W$ of our linear regressor) and the size of the output (i.e. how many scalars it produces) of the layer. We additionaly request our layer to have a bias term $b$.\n",
    "\n",
    "The `Linear` layer processes its input as $XW + b$, which is exactly the (first) equation of the linear regressor we saw before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wo-mlrrdPfYo"
   },
   "outputs": [],
   "source": [
    "class LinearRegressor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.regressor = torch.nn.Linear(in_features=P, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.regressor(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4BsHWq8QpCM"
   },
   "source": [
    "We can create an instance of our model and inspect the current parameters by using the `state_dict` method, which prints the building blocks of our model and their current parameters. Note that `state_dict` is essentially a dictonary indexed by the names of the building blocks which we defined inside the constructor (plus some additional identifiers if a layer has more than one set of parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac6Y3DpDQaxr"
   },
   "outputs": [],
   "source": [
    "lin_reg = LinearRegressor()\n",
    "\n",
    "for param_name, param in lin_reg.state_dict().items():\n",
    "    print(param_name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMfs7q6KQ-kT"
   },
   "source": [
    "We can update the parameters via `state_dict` and re-using the same OLS estimates we obtained before.\n",
    "\n",
    "Note that torch is thought of for Deep Learning: it does not have the routines to solve different ML problems (just use `sklearn` for this).\n",
    "\n",
    "Next time, we'll see how we can unleash gradient-based iterative training routines in torch and compare the results w.r.t. the OLS estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5DbBSqhQvSF"
   },
   "outputs": [],
   "source": [
    "state_dict = lin_reg.state_dict()\n",
    "state_dict[\"regressor.weight\"] = W_star[:P].T\n",
    "state_dict[\"regressor.bias\"] = W_star[P]\n",
    "lin_reg.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEcg3K2HRSba"
   },
   "outputs": [],
   "source": [
    "X_lin_reg = X[:,:P]\n",
    "predictions_lin_reg = lin_reg(X_lin_reg) # equivalent to lin_reg.foward(X_lin_reg)\n",
    "print(torch.nn.functional.mse_loss(predictions_lin_reg, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHUJvpTmqmYv"
   },
   "source": [
    "## Towards gradient-based training of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2V-wW_hqqtl"
   },
   "source": [
    "### Definition of a loss function\n",
    "\n",
    "One key element that we need to train any neural network is a loss function, i.e. a function that quantifies how *good* is our fit to the data and that is differentiable w.r.t. the weights and biases of the network.\n",
    "\n",
    "We saw some examples of common loss functions in the previous lecture, and all the main losses used in Deep Learning are already implemented and available in PyTorch, to cite some:\n",
    "\n",
    "*   `torch.nn.MSELoss`\n",
    "*   `torch.nn.CrossEntropyLoss`\n",
    "*   `torch.nn.BCELoss`\n",
    "*   `nn.KLDivLoss`\n",
    "\n",
    "You can also define your own custom loss function, and as long as you use built-in torch functions to compute it (and you keep it differentiable), you should be fine.\n",
    "\n",
    "For example, you could build your own MSE loss like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEUjpIJBqpzH"
   },
   "outputs": [],
   "source": [
    "def mseloss(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17Kjge2LFXJu"
   },
   "source": [
    "### Definition of a DataLoader object\n",
    "\n",
    "To train any PyTorch model, it is useful to handle data through a `DataLoader` object. A `DataLoader` is an iterable wrapped around a `Dataset` object that allows to easily run through your data in batches.\n",
    "\n",
    "Starting from a set of `Tensor`s representing features and labels, it is easy to define the `Dataset` and its corresponding `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nf982KSbFUpu"
   },
   "outputs": [],
   "source": [
    "dataset=torch.utils.data.TensorDataset(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJnoNnObIDso"
   },
   "outputs": [],
   "source": [
    "dataloader=torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rfKtE_yIIax"
   },
   "outputs": [],
   "source": [
    "X_0, y_0=next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KniM65qIZw3"
   },
   "outputs": [],
   "source": [
    "print(X_0, y_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFtoFMUNMZ00"
   },
   "source": [
    "For any specific need, you can build your own `Dataset` class. To make it work properly, you always have to implement three functions: `__init__`, `__len__` and `__getitem__`. More info on this [here](https://pytorch.org/docs/stable/data.html)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
