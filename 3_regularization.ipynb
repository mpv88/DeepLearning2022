{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inkmEpop3Rb7"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lorenzobasile/DeepLearning2022/blob/main/3_regularization.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3YBQpy13Rb8"
   },
   "source": [
    "# Lab 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFzjKSXX3Rb9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdTjjK2a3Rb-"
   },
   "source": [
    "### Recap from previous Labs\n",
    "\n",
    "* We saw how to handle `torch` tensors and how to use them for our computational purposes;\n",
    "* We defined our first MLP models both for regression and classification;\n",
    "* We now know how to define loss functions, differentiate them and use their gradients to train a simple neural network;\n",
    "* We saw our first (simple) example of a real application of DL: MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NSEKNdb3Rb_"
   },
   "source": [
    "### Today\n",
    "\n",
    "We will introduce the idea of **regularization** and the main techniques employed to regularize neural network learning and to prevent overfitting and we will see the main techniques used to **normalize** the data we feed into a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjyNuL3_5j5L"
   },
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH-1gF3v3Rb_"
   },
   "source": [
    "## What is regularization? Why do we need it?\n",
    "\n",
    "Regularization is the name of a very broad and diverse set of techniques that are used to improve the way neural networks (but, more generally, any ML model) learn. As we will see, not all techniques are based on the same working principle: some insert some sort of bias in the solution that we are looking for, while others directly impact the architecture of the model or the length of the training procedure.\n",
    "\n",
    "What all these techniques have in common is the reason why they are implemented: to avoid or contain *overfitting*.\n",
    "\n",
    "Overfitting is a widespread phenomenon in Machine Learning. It happens when a model captures *too well* the variability of training data, meaning that the model is fitting the noise and irrelevant features of training data, becoming unable to generalize on unseen data (i.e. test data).\n",
    "\n",
    "To have an idea of what overfitting is we can think of a simple regression task:\n",
    "\n",
    "<img src=\"https://github.com/lorenzobasile/DeepLearning2022/blob/main/images/points.png?raw=1\" width=\"500\"/>\n",
    "\n",
    "Since we have 10 points there exists a unique polynomial of degree 9 that passes through these points, perfectly fitting (overfitting) our data:\n",
    "\n",
    "<img src=\"https://github.com/lorenzobasile/DeepLearning2022/blob/main/images/poly.png?raw=1\" width=\"500\"/>\n",
    "\n",
    "Paying a small price in terms of MSE, we could obtain a decent fit with a linear model (polynomial of degree 1):\n",
    "\n",
    "<img src=\"https://github.com/lorenzobasile/DeepLearning2022/blob/main/images/lin.png?raw=1\" width=\"500\"/>\n",
    "\n",
    "It is obvious not to decide which model works best: in principle nothing is telling us that we shouldn't fit a polynomial of degree 9 for this task. However, in general we strive for simpler models, as they are clearer, more explainable and better at capturing general patterns in the data, which usually makes them better at generalizing on unseen data points.\n",
    "\n",
    "\n",
    "\n",
    "Please refer to [chapter 3](http://neuralnetworksanddeeplearning.com/chap3.html) of Michael Nielsen's book for a more in-depth analysis of this example and of some of the techniques we will see in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOEY7rzY3Rb_"
   },
   "source": [
    "## A set of tools for regularization\n",
    "\n",
    "Regularization in DL comes in the form of different tools. We can have:\n",
    "\n",
    "1. Penalty terms in loss functions (e.g. L1 and L2 norm regularization) which introduce bias in our parameters by actively reducing the magnitude of some weights:\n",
    "    * L2 norm regularization is also called Ridge regularization or **weight decay**\n",
    "    * L1 norm regularization is also called LASSO regularization\n",
    "    * they were originally implemented in linear regression models as a way to infuse *inductive bias* in models originally thought to rely on the complete unbiasedness on training data\n",
    "2. Dropout, a technique [patented by Google](https://patents.google.com/patent/US9406017B2/en) which consists in randomly *dropping* some neurons from a given layer to prevent overfitting.\n",
    "3. Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciRmsbqw5j5M"
   },
   "source": [
    "## The XOR problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCokOd8r5j5N"
   },
   "source": [
    "We will be dealing with a simple toy problem that is very well known in the Deep Learning community: the XOR problem.\n",
    "\n",
    "It is a binary classification problem in which the input is 2-dimensional and the output is 1 if the two signs coincide and 0 otherwise. The problem is trivial but it draws importance from the fact that it is one of the simplest classification tasks that cannot be solved by using a linear model.\n",
    "\n",
    "We add some noise in the data by randomly switching the label for 5% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPS8UM1Gaobh"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "X=torch.randn((1000,2))\n",
    "Y=torch.logical_or(torch.all(X.ge(0), dim=1), torch.all(X.le(0), dim=1)).reshape(-1,1)\n",
    "\n",
    "p=int(0.05*Y.shape[0])\n",
    "perm = torch.randperm(Y.shape[0])\n",
    "Y[perm[:p]]=~Y[perm[:p]]\n",
    "Y=Y.float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0mYOSPa5j5N"
   },
   "source": [
    "We can visualize the data belonging to the two classes: it is clearly visible that it is impossible to separate the classes with a linear boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOzuFlcKaoeH"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X[:,0], X[:,1], s=10, c=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3NjbTng5j5O"
   },
   "source": [
    "We can use a PyTorch built-in function to randomly split the dataset into a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOQEuad2cj8A"
   },
   "outputs": [],
   "source": [
    "dataset=torch.utils.data.TensorDataset(X, Y)\n",
    "trainset, testset=torch.utils.data.random_split(dataset, [500, 500])\n",
    "\n",
    "trainloader=torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "testloader=torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2kMynlM5j5O"
   },
   "source": [
    "And we define our binary classifier as an MLP with three layers, ReLU activations and a final Sigmoid activation. We will be using the `BCELoss` (the binary version of Cross Entropy): please note that this loss function in torch requires the output to be 1D and normalized, i.e. **the Sigmoid is needed** (recall that Softmax is not needed when doing multi-class classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSGXV2rD3RcB"
   },
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim=2, linear=False):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(in_features=input_dim, out_features=4, bias=True)\n",
    "        self.layer2 = torch.nn.Linear(in_features=4, out_features=4, bias=True)\n",
    "        self.layer3 = torch.nn.Linear(in_features=4, out_features=1, bias=True)\n",
    "        if linear:\n",
    "            self.activation = torch.nn.Identity()\n",
    "        else:\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.activation(self.layer1(x))\n",
    "        x=self.activation(self.layer2(x))\n",
    "        x=self.sigmoid(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaIP57nD3RcA"
   },
   "source": [
    "## Weight decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UlApQzl5j5P"
   },
   "source": [
    "The first kind of regularization we will deal with is Ridge regularization (or L2 regularization, or simply Weight Decay).\n",
    "\n",
    "Weight Decay  is a simple technique which *appends* a penalty term to the loss function equation. The term is based upon the $l_2$ norm of the weights.\n",
    "\n",
    "Given our original loss function $L (\\hat{y}, y)$ and our parameter vector $\\Theta$, our new loss will be:\n",
    "\n",
    "$$\n",
    "L (\\hat{y}, y) + \\lambda \\cdot \\vert\\vert \\Theta \\vert\\vert_2^2\n",
    "$$\n",
    "\n",
    "the parameter $\\lambda$ (also called weight decay) controls the strength of the regularization. $\\lambda$ too high means that the model will not concentrate well enough on the original objective ($L$), hence it will not perform well.\n",
    "\n",
    "In torch, instead of inserting our penalty term in the loss function, we specify the weight decay parameter in our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWn3qtLv3RcB"
   },
   "outputs": [],
   "source": [
    "l2_model=Classifier()\n",
    "optimizer=torch.optim.Adam(l2_model.parameters(), lr=1e-2, weight_decay=0.01)\n",
    "loss=torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amPO2Utk3RcB"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct=0\n",
    "        for x, y in iter(dataloader):\n",
    "            out=model(x)\n",
    "            correct+=(torch.round(out)==y).sum()\n",
    "        return correct/len(dataloader.dataset)\n",
    "def get_l2_norm(model):\n",
    "    total_norm=0\n",
    "    for p in model.parameters():\n",
    "        param_norm = torch.norm(p.data, p=2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    return total_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EQwJo2I5j5P"
   },
   "source": [
    "We can quickly obtain good performance on our task while limiting the weight norm increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NO-3nZC5j5Q"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, trainloader, testloader):\n",
    "    epochs=10\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Test accuracy: \", get_accuracy(model, testloader))\n",
    "        model.train()    \n",
    "        print(\"Epoch: \", epoch)\n",
    "        for x, y in iter(trainloader):\n",
    "            out=model(x)\n",
    "            l=loss(out, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "    print(\"Final accuracy: \", get_accuracy(model, testloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KbwMQ7q3RcB"
   },
   "outputs": [],
   "source": [
    "train(l2_model, optimizer, trainloader, testloader)\n",
    "print(\"Final norm: \", get_l2_norm(l2_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIUMn0795j5Q"
   },
   "source": [
    "What would happen if we replace the `ReLU` with `Identity`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9bRGuFF5j5Q"
   },
   "outputs": [],
   "source": [
    "linear_model=Classifier(linear=True)\n",
    "optimizer=torch.optim.Adam(linear_model.parameters(), lr=1e-2, weight_decay=0.01)\n",
    "train(linear_model, optimizer, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHOp1tH-3RcC"
   },
   "source": [
    "## LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjMg4UQv5j5Q"
   },
   "source": [
    "\n",
    "LASSO (or L1 norm) regularization is analogous to weight decay. The equation is:\n",
    "\n",
    "$$\n",
    "L (\\hat{y}, y) + \\lambda \\cdot \\vert\\vert \\Theta \\vert\\vert_1\n",
    "$$\n",
    "\n",
    "where $\\vert\\vert x \\vert\\vert_1 = \\sum_{j=1}^d \\vert x_j \\vert$\n",
    "\n",
    "unlike weight decay, torch does not provide a built-in for L1 regularization. To implement it you would have to customize the optimizer, create a custom loss function or simply tweak the training loop as we will see.\n",
    "\n",
    "L1 regularization is often used as a way to enforce *sparsity* in the model weights: in fact, $l_1$ norm is used as a differentiable proxy  for the so-called $l_0$ norm of the weights, defined as the number of nonzero elements of a vector:\n",
    "\n",
    "$$\\vert\\vert x \\vert\\vert_0 = \\sum_{j=1}^d  \\delta(x_j \\neq 0) $$\n",
    "\n",
    "For example, recalling the regression example we saw before, L1 regularization would come in handy to sparsify a model of degree 9 (having 10 parameters) to make it a linear model. We can obtain a similar situation in our XOR problem if we add some meaningless features to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHxWk7Ko5j5R"
   },
   "outputs": [],
   "source": [
    "new_column=torch.randn((X.shape[0],3))\n",
    "X_noisy=torch.cat([X, new_column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-V2KpbxE5j5R"
   },
   "outputs": [],
   "source": [
    "dataset=torch.utils.data.TensorDataset(X_noisy, Y)\n",
    "trainset_noisy, testset_noisy=torch.utils.data.random_split(dataset, [500, 500])\n",
    "\n",
    "trainloader_noisy=torch.utils.data.DataLoader(trainset_noisy, batch_size=16, shuffle=True)\n",
    "testloader_noisy=torch.utils.data.DataLoader(testset_noisy, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9ltaLLD5j5R"
   },
   "source": [
    "Now the input layer of the classifier should have shape 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YU4FJ21fFN-o"
   },
   "outputs": [],
   "source": [
    "l1_model=Classifier(5)\n",
    "lam=0.001\n",
    "optimizer=torch.optim.Adam(l1_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYPrEUPBFU2a"
   },
   "outputs": [],
   "source": [
    "epochs=10\n",
    "losses=[]\n",
    "test_accuracies=[]\n",
    "train_accuracies=[]\n",
    "for epoch in range(epochs):\n",
    "    print(\"Test accuracy: \", get_accuracy(l1_model, testloader_noisy))\n",
    "    l1_model.train()\n",
    "    print(\"l1 norm: \", sum(p.abs().sum() for p in l1_model.parameters()))\n",
    "    print(\"l0 norm: \", sum(torch.sum(p.abs().ge(1e-2)) for p in l1_model.parameters()))\n",
    "    \n",
    "    print(\"Epoch: \", epoch)\n",
    "    for x, y in iter(trainloader_noisy):\n",
    "        out=l1_model(x)\n",
    "        l=loss(out, y)\n",
    "        l1_norm=sum(p.abs().sum() for p in l1_model.parameters())\n",
    "        l+=lam*l1_norm\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(l.item())\n",
    "print(\"Final accuracy: \", get_accuracy(l1_model, testloader_noisy))\n",
    "print(\"Final l1 norm: \", sum(p.abs().sum() for p in l1_model.parameters()))\n",
    "print(\"Final l0 norm: \", sum(torch.sum(p.abs().ge(1e-2)) for p in l1_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeraFQtp5j5R"
   },
   "outputs": [],
   "source": [
    "for p in l1_model.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri9SjR6G3RcC"
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hrvxg_wg5j5R"
   },
   "source": [
    "\n",
    "Dropout acts by removing (i.e. *zeroing-out*) a random subset of the neurons in a given layer for each forward pass.\n",
    "\n",
    "<img src=\"https://github.com/lorenzobasile/DeepLearning2022/blob/main/images/dropout.png?raw=1\" width=\"500\"/>\n",
    "\n",
    "It has one hyperparameter ($p$), which is the fraction of neurons to be dropped out.\n",
    "\n",
    "During training, each time a layer with backprop produces an output, a fraction $p$ of that output gets discarded (more precisely, the output of each neuron gets discarded w.p. $p$). This helps in such a way that co-dependence between neurons gets *forgotten* by the network. To say it in simple terms, it forces each neuron to be independent from the output of other neurons within the same layer.\n",
    "\n",
    "Be careful: since dropout has to apply only during training, we must be careful in activating the switch `model.eval()` when testing our network.\n",
    "\n",
    "In torch, we find Dropout as a module of `torch.nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OM82OtQ5j5S"
   },
   "outputs": [],
   "source": [
    "class DropoutClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(in_features=2, out_features=16, bias=True)\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        self.layer2 = torch.nn.Linear(in_features=16, out_features=8, bias=True)\n",
    "        self.layer3 = torch.nn.Linear(in_features=8, out_features=1, bias=True)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, show=False):\n",
    "        x=self.activation(self.layer1(x))\n",
    "        if show:\n",
    "            print(\"Before Dropout\")\n",
    "            print(x)\n",
    "        x=self.dropout(x)\n",
    "        if show:\n",
    "            print(\"After Dropout\")\n",
    "            print(x)\n",
    "        x=self.activation(self.layer2(x))\n",
    "        x=self.sigmoid(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmDD7iiS5j5S"
   },
   "outputs": [],
   "source": [
    "do_model=DropoutClassifier()\n",
    "optimizer=torch.optim.Adam(do_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjH8ZuPs5j5S"
   },
   "outputs": [],
   "source": [
    "train(do_model, optimizer, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "druV5NsH5j5S"
   },
   "source": [
    "We can visualize the effect of Dropout in training and evaluation modes. Note that Dropout outputs are always rescaled of a factor $\\frac{1}{1-p}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5x1YOy6l5j5S"
   },
   "outputs": [],
   "source": [
    "x,y=next(iter(trainloader))\n",
    "\n",
    "do_model.train()\n",
    "do_model(x[0], True)\n",
    "\n",
    "do_model.eval()\n",
    "do_model(x[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-zyA8cI3RcC"
   },
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6P7_Pnx5j5S"
   },
   "source": [
    "Early stopping is yet another example of regularization technique which relies a lot on practical and experimental observations rather than any supporting theory.\n",
    "\n",
    "It is based upon the concept of **validation**, which is an assessment mode **additional** to *testing*. Actually, what insofar whe have described as testing is technically a validation.\n",
    "* a validation dataset may be obtained as result of a random splitting of the original training dataset\n",
    "* a testing dataset should be obtained instead from a model deployed \"in the wild\" and should consist of data unseen (from both the model and its architect) during the training and designing phase.\n",
    "\n",
    "In a normal academic setting it's very hard to obtain a proper testing dataset, so usually the meaning of testing and validation get mixed up a little bit.\n",
    "\n",
    "Anyway, early stopping requires us to assess the model at each epoch to get a proxy for the testing performance(s) (**validation step**). That should gives us an idea of how the model **learns to generalize** (if it ever does...) during training.\n",
    "\n",
    "The *theoretical trend* ('90 s), which is pretty much absent in modern Deep Learning due to a lot of modern factors, is the following:\n",
    "\n",
    "<img src=\"https://github.com/lorenzobasile/DeepLearning2022/blob/main/images/generalization.jpg?raw=1\" width=\"500\"/>\n",
    "\n",
    "The idea of early stopping comes from the fact that it is not obvious that more training epochs means better performances: there can be a sweet spot in which we are not at the minimum of the training loss but we are at the optimal point in terms of generalization on the validation set. Then it is a good idea to interrupt the training and fix the model weights for evaluation.\n",
    "\n",
    "There are many possible criteria to decide when to stop exactly: you can find a complete discussion [here](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf). The simplest criteria stop training when the generalization loss (i.e. the validation loss) is stable or increasing for a number of consecutive epochs.\n",
    "\n",
    "As anticipated, modern Deep Learning practise suggests that early stopping is not always a good idea, since the trend of the previous plot is often not visible. Something like this can happen instead ([Double Descent Phenomenon](https://openai.com/blog/deep-double-descent/)):\n",
    "\n",
    "<img src=\"https://github.com/lorenzobasile/DeepLearning2022/blob/main/images/dd.png?raw=1\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7Z3Et_i5j5T"
   },
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AE2mokpr5j5T"
   },
   "source": [
    "Now, we will move to normalization techniques, which are aimed at transforming features to be on a similar scale to improve the performance and training stability of the model.\n",
    "\n",
    "We will have a look at [Batch Normalization (BatchNorm)](https://arxiv.org/abs/1502.03167) and [Layer Normalization (LayerNorm)](https://arxiv.org/pdf/1607.06450.pdf). These two techniques follow the same procedure, rescaling data to have mean 0 and variance 1, but work on different axes:\n",
    "\n",
    "<img src=\"https://github.com/lorenzobasile/DeepLearning2022/blob/main/images/normalization.png?raw=1\" width=\"500\"/>\n",
    "\n",
    "Choosing between BatchNorm and LayerNorm (or no normalization layer at all) is a hard task, as there is no fixed rule to do it. As a rule of thumb, Computer Vision architectures often employ BatchNorm and NLP ones LayerNorm, but this is highly variable and the situation is changing with the widespread success of Transformer architectures (originally intended for NLP) on different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPfehPsh3RcC"
   },
   "source": [
    "### Batch Normalization\n",
    "\n",
    "In torch, BatchNorm and LayerNorm are implemented as standard layers in the `nn` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmPllhvL5j5T"
   },
   "outputs": [],
   "source": [
    "class BNClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(in_features=2, out_features=16, bias=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(num_features=16)\n",
    "        self.layer2 = torch.nn.Linear(in_features=16, out_features=8, bias=True)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(num_features=8)\n",
    "        self.layer3 = torch.nn.Linear(in_features=8, out_features=1, bias=True)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.bn1(self.activation(self.layer1(x)))\n",
    "        x=self.bn2(self.activation(self.layer2(x)))\n",
    "        x=self.sigmoid(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9X5k8c-25j5T"
   },
   "source": [
    "Note that `BatchNorm1d` assumes a 2-D or 3-D input. If you are handling images, which are 4-D (batch size, channel, height, width) you can switch to `BatchNorm2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aD0SHNGk5j5T"
   },
   "outputs": [],
   "source": [
    "bn_model=BNClassifier()\n",
    "optimizer=torch.optim.Adam(bn_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1piDV6Hl5j5T"
   },
   "outputs": [],
   "source": [
    "train(bn_model, optimizer, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Akv8bFG83RcC"
   },
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZK_Ainx5j5T"
   },
   "source": [
    "LayerNorm works in a very similar way as BatchNorm, and it accepts as input the shape of the slice on which you want to compute mean and std: for an image it would be something like `normalized_shape=[C,H,W]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MBzWYI05j5T"
   },
   "outputs": [],
   "source": [
    "class LNClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(in_features=2, out_features=16, bias=True)\n",
    "        self.ln1 = torch.nn.LayerNorm(normalized_shape=16)\n",
    "        self.layer2 = torch.nn.Linear(in_features=16, out_features=8, bias=True)\n",
    "        self.ln2 = torch.nn.LayerNorm(normalized_shape=8)\n",
    "        self.layer3 = torch.nn.Linear(in_features=8, out_features=1, bias=True)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.ln1(self.activation(self.layer1(x)))\n",
    "        x=self.ln2(self.activation(self.layer2(x)))\n",
    "        x=self.sigmoid(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4iIqKPGM5j5U"
   },
   "outputs": [],
   "source": [
    "ln_model=LNClassifier()\n",
    "optimizer=torch.optim.Adam(ln_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETpTbdiG5j5U"
   },
   "outputs": [],
   "source": [
    "train(ln_model, optimizer, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_LnNC-I8bBM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
